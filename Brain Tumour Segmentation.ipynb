{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9174523,"sourceType":"datasetVersion","datasetId":5544596}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown","metadata":{"id":"b9zy6hnV102H","execution":{"iopub.status.busy":"2024-09-04T13:42:01.659056Z","iopub.status.idle":"2024-09-04T13:42:15.116199Z","shell.execute_reply.started":"2024-09-04T13:42:01.659691Z","shell.execute_reply":"2024-09-04T13:42:15.115185Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.7.4)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import gdown","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:42:15.118043Z","iopub.execute_input":"2024-09-04T13:42:15.118345Z","iopub.status.idle":"2024-09-04T13:42:15.406341Z","shell.execute_reply.started":"2024-09-04T13:42:15.118317Z","shell.execute_reply":"2024-09-04T13:42:15.405601Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"url = 'https://drive.google.com/uc?id=1PpjMOsbXxagL809U46RaTPU_Kc2alE3k'\noutput = 'BrainTumour.tar'\ngdown.download(url, output, quiet=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:42:34.721103Z","iopub.execute_input":"2024-09-04T13:42:34.721468Z","iopub.status.idle":"2024-09-04T13:43:25.456092Z","shell.execute_reply.started":"2024-09-04T13:42:34.721425Z","shell.execute_reply":"2024-09-04T13:43:25.455169Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1PpjMOsbXxagL809U46RaTPU_Kc2alE3k\nFrom (redirected): https://drive.google.com/uc?id=1PpjMOsbXxagL809U46RaTPU_Kc2alE3k&confirm=t&uuid=6161aeb4-cd8b-4aab-9a9f-560131cba791\nTo: /kaggle/working/BrainTumour.tar\n100%|██████████| 7.61G/7.61G [00:46<00:00, 164MB/s] \n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'BrainTumour.tar'"},"metadata":{}}]},{"cell_type":"code","source":"import tarfile\n\ndef extract_tar_file(tar_file_path, extract_path='.'):\n    try:\n        with tarfile.open(tar_file_path, 'r') as tar:\n            tar.extractall(path=extract_path)\n            print(f\"Extracted all contents of {tar_file_path} to {extract_path}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\ntar_file_path = '/kaggle/working/BrainTumour.tar'\nextract_path = '/kaggle/working/'\nextract_tar_file(tar_file_path, extract_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:44:09.635141Z","iopub.execute_input":"2024-09-04T13:44:09.635490Z","iopub.status.idle":"2024-09-04T13:44:24.425579Z","shell.execute_reply.started":"2024-09-04T13:44:09.635461Z","shell.execute_reply":"2024-09-04T13:44:24.424594Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Extracted all contents of /kaggle/working/BrainTumour.tar to /kaggle/working/\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Specify the file path\nfile_path = 'BrainTumour.tar'\n\n# Check if the file exists before attempting to delete it\nif os.path.exists(file_path):\n    os.remove(file_path)\n    print(f\"{file_path} has been deleted.\")\nelse:\n    print(f\"{file_path} does not exist.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:45:17.204839Z","iopub.execute_input":"2024-09-04T13:45:17.205205Z","iopub.status.idle":"2024-09-04T13:45:17.952034Z","shell.execute_reply.started":"2024-09-04T13:45:17.205177Z","shell.execute_reply":"2024-09-04T13:45:17.951105Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"BrainTumour.tar has been deleted.\n","output_type":"stream"}]},{"cell_type":"code","source":"import nibabel as nib\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport tarfile\nimport numpy as np\nimport os\nfrom glob import glob","metadata":{"id":"zvzr0pzv1sGq","execution":{"iopub.status.busy":"2024-09-04T13:45:21.111002Z","iopub.execute_input":"2024-09-04T13:45:21.111355Z","iopub.status.idle":"2024-09-04T13:45:21.312507Z","shell.execute_reply.started":"2024-09-04T13:45:21.111325Z","shell.execute_reply":"2024-09-04T13:45:21.311782Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!pip install monai","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wd_xiHjk1uF6","outputId":"a20d074e-2997-45f5-8f3a-89d4e3b65746","execution":{"iopub.status.busy":"2024-09-04T13:45:25.552614Z","iopub.execute_input":"2024-09-04T13:45:25.553283Z","iopub.status.idle":"2024-09-04T13:45:40.050463Z","shell.execute_reply.started":"2024-09-04T13:45:25.553251Z","shell.execute_reply":"2024-09-04T13:45:40.049531Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Collecting monai\n  Downloading monai-1.3.2-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.10/site-packages (from monai) (2.1.2)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from monai) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (2024.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->monai) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9->monai) (1.3.0)\nDownloading monai-1.3.2-py3-none-any.whl (1.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: monai\nSuccessfully installed monai-1.3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from monai.utils import set_determinism","metadata":{"id":"s8BVq8dqBLLe","execution":{"iopub.status.busy":"2024-09-04T13:45:40.052435Z","iopub.execute_input":"2024-09-04T13:45:40.052765Z","iopub.status.idle":"2024-09-04T13:46:21.389487Z","shell.execute_reply.started":"2024-09-04T13:45:40.052736Z","shell.execute_reply":"2024-09-04T13:46:21.388487Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"2024-09-04 13:46:12.743182: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-04 13:46:12.743282: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-04 13:46:12.864360: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"os.mkdir('/kaggle/working/TrainVolumes')\nos.mkdir('/kaggle/working/TrainSegmentation')\nos.mkdir('/kaggle/working/TestVolumes')\nos.mkdir('/kaggle/working/TestSegmentation')","metadata":{"id":"Pl88WdPeY45b","execution":{"iopub.status.busy":"2024-09-04T13:46:21.390952Z","iopub.execute_input":"2024-09-04T13:46:21.391635Z","iopub.status.idle":"2024-09-04T13:46:21.396571Z","shell.execute_reply.started":"2024-09-04T13:46:21.391607Z","shell.execute_reply":"2024-09-04T13:46:21.395560Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import shutil\n\nfor i in range(1, 10):\n  shutil.copy(f'/kaggle/working/Task01_BrainTumour/imagesTr/BRATS_00{i}.nii.gz', '/kaggle/working/TrainVolumes')\n\nfor i in range(1, 10):\n  shutil.copy(f'/kaggle/working/Task01_BrainTumour/labelsTr/BRATS_00{i}.nii.gz', '/kaggle/working/TrainSegmentation')\n\nfor i in range(10, 100):\n  shutil.copy(f'/kaggle/working/Task01_BrainTumour/imagesTr/BRATS_0{i}.nii.gz', '/kaggle/working//TrainVolumes')\n\nfor i in range(10, 100):\n  shutil.copy(f'/kaggle/working/Task01_BrainTumour/labelsTr/BRATS_0{i}.nii.gz', '/kaggle/working/TrainSegmentation')\n\nfor i in range(100, 401):\n  shutil.copy(f'/kaggle/working/Task01_BrainTumour/imagesTr/BRATS_{i}.nii.gz', '/kaggle/working//TrainVolumes')\n\nfor i in range(100, 401):\n  shutil.copy(f'/kaggle/working/Task01_BrainTumour/labelsTr/BRATS_{i}.nii.gz', '/kaggle/working/TrainSegmentation')\n\n","metadata":{"id":"6RE83mfkQuWq","execution":{"iopub.status.busy":"2024-09-04T13:46:21.399367Z","iopub.execute_input":"2024-09-04T13:46:21.399723Z","iopub.status.idle":"2024-09-04T13:46:24.617645Z","shell.execute_reply.started":"2024-09-04T13:46:21.399694Z","shell.execute_reply":"2024-09-04T13:46:24.616523Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"for i in range(401, 451):\n  shutil.copy(f'/kaggle/working/Task01_BrainTumour/imagesTr/BRATS_{i}.nii.gz', '/kaggle/working/TestVolumes')\n\nfor i in range(401, 451):\n  shutil.copy(f'/kaggle/working/Task01_BrainTumour/labelsTr/BRATS_{i}.nii.gz', '/kaggle/working/TestSegmentation')\n","metadata":{"id":"1ubGEBiVDOc3","execution":{"iopub.status.busy":"2024-09-04T13:46:24.619009Z","iopub.execute_input":"2024-09-04T13:46:24.619369Z","iopub.status.idle":"2024-09-04T13:46:25.064643Z","shell.execute_reply.started":"2024-09-04T13:46:24.619332Z","shell.execute_reply":"2024-09-04T13:46:25.063717Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from monai.transforms import Compose, LoadImaged, Resized, ToTensord, Spacingd, Orientationd, ScaleIntensityRanged, CropForegroundd, EnsureChannelFirstd\nfrom monai.data import DataLoader, Dataset, CacheDataset\nfrom monai.utils import set_determinism, first\nimport matplotlib.pyplot as plt","metadata":{"id":"9sNECdTi1wyh","execution":{"iopub.status.busy":"2024-09-04T13:46:25.066204Z","iopub.execute_input":"2024-09-04T13:46:25.066506Z","iopub.status.idle":"2024-09-04T13:46:25.271613Z","shell.execute_reply.started":"2024-09-04T13:46:25.066480Z","shell.execute_reply":"2024-09-04T13:46:25.270421Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def prepare(in_dir, pixdim = (1.5, 1.5, 1.0),  spatial_size =[128, 128, 128], cache  = False):\n  \"\"\"\n  Preprocessing Function\n  \"\"\"\n\n  set_determinism(seed=0)\n\n  path_train_volumes = sorted(glob(os.path.join(in_dir, 'TrainVolumes', \"*.nii.gz\")))\n  path_train_segmentation = sorted(glob(os.path.join(in_dir, \"TrainSegmentation\", \"*.nii.gz\")))\n\n  path_test_volume = sorted(glob(os.path.join(in_dir, 'TestVolumes', \"*.nii.gz\")))\n  path_test_segmentation = sorted(glob(os.path.join(in_dir, \"TestSegmentation\", \"*.nii.gz\")))\n\n  train_files = [{\"vol\": image_name, \"seg\": label_name} for image_name, label_name in\n                 zip(path_train_volumes, path_train_segmentation)]\n  test_files = [{\"vol\": image_name, \"seg\": label_name} for image_name, label_name in\n                zip(path_test_volume, path_test_segmentation)]\n\n  train_transforms = Compose(\n      [\n          LoadImaged(keys=[\"vol\", \"seg\"]),\n          EnsureChannelFirstd(keys=[\"vol\", \"seg\"]),\n          Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n          Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n          CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n          CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"seg\"),\n          Resized(keys=[\"vol\", \"seg\"], spatial_size=spatial_size),\n          ToTensord(keys=[\"vol\", \"seg\"])\n\n      ]\n  )\n\n  test_transforms = Compose([\n      LoadImaged(keys=[\"vol\", \"seg\"]),\n      EnsureChannelFirstd(keys=[\"vol\", \"seg\"]),\n      Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n      Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n      CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n      Resized(keys=[\"vol\", \"seg\"], spatial_size=spatial_size),\n      ToTensord(keys=[\"vol\", \"seg\"])\n  ])\n\n  if cache:\n        train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0)\n        train_loader = DataLoader(train_ds, batch_size=1)\n\n        test_ds = CacheDataset(data=test_files, transform=test_transforms, cache_rate=1.0)\n        test_loader = DataLoader(test_ds, batch_size=1)\n\n        return train_loader, test_loader\n\n  else:\n        train_ds = Dataset(data=train_files, transform=train_transforms)\n        train_loader = DataLoader(train_ds, batch_size=1)\n\n        test_ds = Dataset(data=test_files, transform=test_transforms)\n        test_loader = DataLoader(test_ds, batch_size=1)\n\n        return train_loader, test_loader\n\n\n","metadata":{"id":"DtOZsJkJ79Kt","execution":{"iopub.status.busy":"2024-09-04T13:46:25.273259Z","iopub.execute_input":"2024-09-04T13:46:25.273654Z","iopub.status.idle":"2024-09-04T13:46:26.078224Z","shell.execute_reply.started":"2024-09-04T13:46:25.273619Z","shell.execute_reply":"2024-09-04T13:46:26.076809Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nimport os\nimport numpy as np\nimport numpy as np\nfrom monai.losses import DiceLoss\nfrom tqdm import tqdm\n","metadata":{"id":"63afQWzsTeK8","execution":{"iopub.status.busy":"2024-09-04T13:46:26.079433Z","iopub.execute_input":"2024-09-04T13:46:26.080126Z","iopub.status.idle":"2024-09-04T13:46:27.458470Z","shell.execute_reply.started":"2024-09-04T13:46:26.080099Z","shell.execute_reply":"2024-09-04T13:46:27.457619Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\ndef dice_metric(predicted, target):\n    '''\n    In this function we take `predicted` and `target` (label) to calculate the dice coeficient then we use it\n    to calculate a metric value for the training and the validation.\n    '''\n    dice_value = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\n    value = 1 - dice_value(predicted, target).item()\n    return value\n\ndef hausdorff_metric(predicted, target):\n    \"\"\"\n    This function calculates the Hausdorff distance between `predicted` and `target` (label).\n    It returns a metric value for training and validation.\n    \"\"\"\n    # Initialize Hausdorff Distance Metric with 95th percentile\n    hd_metric = HausdorffDistanceMetric(include_background=True, percentile=95)\n    \n    # Compute the Hausdorff distance\n    hausdorff_distance = hd_metric(predicted, target)\n    \n    # Return the computed distance\n    return hausdorff_distance.item()\n\n\ndef calculate_weights(val1, val2):\n    '''\n    In this function we take the number of the background and the forgroud pixels to return the `weights`\n    for the cross entropy loss values.\n    '''\n    count = np.array([val1, val2])\n    summ = count.sum()\n    weights = count/summ\n    weights = 1/weights\n    summ = weights.sum()\n    weights = weights/summ\n    return torch.tensor(weights, dtype=torch.float32)\n\ndef train(model, data_in, loss, optim, max_epochs, model_dir, test_interval=1 , device=torch.device(\"cuda:0\")):\n    best_metric = -1\n    best_metric_epoch = -1\n    save_loss_train = []\n    save_loss_test = []\n    save_metric_train = []\n    save_metric_test = []\n    train_loader, test_loader = data_in\n\n    for epoch in range(max_epochs):\n        print(\"-\" * 10)\n        print(f\"epoch {epoch + 1}/{max_epochs}\")\n        model.train()\n        train_epoch_loss = 0\n        train_step = 0\n        epoch_metric_train = 0\n        for batch_data in train_loader:\n\n            train_step += 1\n\n            volume = batch_data[\"vol\"]\n            label = batch_data[\"seg\"]\n            label = label != 0\n            volume, label = (volume.to(device), label.to(device))\n\n            optim.zero_grad()\n            outputs = model(volume)\n\n            train_loss = loss(outputs, label)\n\n            train_loss.backward()\n            optim.step()\n\n            train_epoch_loss += train_loss.item()\n            print(\n                f\"{train_step}/{len(train_loader) // train_loader.batch_size}, \"\n                f\"Train_loss: {train_loss.item():.4f}\")\n\n            train_metric = dice_metric(outputs, label)\n            epoch_metric_train += train_metric\n            print(f'Train_dice: {train_metric:.4f}')\n\n        print('-'*20)\n\n        train_epoch_loss /= train_step\n        print(f'Epoch_loss: {train_epoch_loss:.4f}')\n        save_loss_train.append(train_epoch_loss)\n        np.save(os.path.join(model_dir, 'loss_train.npy'), save_loss_train)\n\n        epoch_metric_train /= train_step\n        print(f'Epoch_metric: {epoch_metric_train:.4f}')\n\n        save_metric_train.append(epoch_metric_train)\n        np.save(os.path.join(model_dir, 'metric_train.npy'), save_metric_train)\n\n        if (epoch + 1) % test_interval == 0:\n\n            model.eval()\n            with torch.no_grad():\n                test_epoch_loss = 0\n                test_metric = 0\n                epoch_metric_test = 0\n                test_step = 0\n\n                for test_data in test_loader:\n\n                    test_step += 1\n\n                    test_volume = test_data[\"vol\"]\n                    test_label = test_data[\"seg\"]\n                    test_label = test_label != 0\n                    test_volume, test_label = (test_volume.to(device), test_label.to(device),)\n\n                    test_outputs = model(test_volume)\n\n                    test_loss = loss(test_outputs, test_label)\n                    test_epoch_loss += test_loss.item()\n                    test_metric = dice_metric(test_outputs, test_label)\n                    epoch_metric_test += test_metric\n\n\n                test_epoch_loss /= test_step\n                print(f'test_loss_epoch: {test_epoch_loss:.4f}')\n                save_loss_test.append(test_epoch_loss)\n                np.save(os.path.join(model_dir, 'loss_test.npy'), save_loss_test)\n\n                epoch_metric_test /= test_step\n                print(f'test_dice_epoch: {epoch_metric_test:.4f}')\n                save_metric_test.append(epoch_metric_test)\n                np.save(os.path.join(model_dir, 'metric_test.npy'), save_metric_test)\n\n                if epoch_metric_test > best_metric:\n                    best_metric = epoch_metric_test\n                    best_metric_epoch = epoch + 1\n                    torch.save(model.state_dict(), os.path.join(\n                        model_dir, \"best_metric_model.pth\"))\n\n                print(\n                    f\"current epoch: {epoch + 1} current mean dice: {test_metric:.4f}\"\n                    f\"\\nbest mean dice: {best_metric:.4f} \"\n                    f\"at epoch: {best_metric_epoch}\"\n                )\n\n\n    print(\n        f\"train completed, best_metric: {best_metric:.4f} \"\n        f\"at epoch: {best_metric_epoch}\")\n\n\ndef show_patient(data, SLICE_NUMBER=1, train=True, test=False):\n    \"\"\"\n    This function is to show one patient from your datasets, so that you can si if the it is okay or you need\n    to change/delete something.\n\n    `data`: this parameter should take the patients from the data loader, which means you need to can the function\n    prepare first and apply the transforms that you want after that pass it to this function so that you visualize\n    the patient with the transforms that you want.\n    `SLICE_NUMBER`: this parameter will take the slice number that you want to display/show\n    `train`: this parameter is to say that you want to display a patient from the training data (by default it is true)\n    `test`: this parameter is to say that you want to display a patient from the testing patients.\n    \"\"\"\n\n    check_patient_train, check_patient_test = data\n\n    view_train_patient = first(check_patient_train)\n    view_test_patient = first(check_patient_test)\n\n\n    if train:\n        plt.figure(\"Visualization Train\", (12, 6))\n        plt.subplot(1, 2, 1)\n        plt.title(f\"vol {SLICE_NUMBER}\")\n        plt.imshow(view_train_patient[\"vol\"][0, 0, :, :, SLICE_NUMBER], cmap=\"gray\")\n\n        plt.subplot(1, 2, 2)\n        plt.title(f\"seg {SLICE_NUMBER}\")\n        plt.imshow(view_train_patient[\"seg\"][0, 0, :, :, SLICE_NUMBER])\n        plt.show()\n\n    if test:\n        plt.figure(\"Visualization Test\", (12, 6))\n        plt.subplot(1, 2, 1)\n        plt.title(f\"vol {SLICE_NUMBER}\")\n        plt.imshow(view_test_patient[\"vol\"][0, 0, :, :, SLICE_NUMBER], cmap=\"gray\")\n\n        plt.subplot(1, 2, 2)\n        plt.title(f\"seg {SLICE_NUMBER}\")\n        plt.imshow(view_test_patient[\"seg\"][0, 0, :, :, SLICE_NUMBER])\n        plt.show()\n\n\ndef calculate_pixels(data):\n    val = np.zeros((1, 2))\n\n    for batch in tqdm(data):\n        batch_label = batch[\"seg\"] != 0\n        _, count = np.unique(batch_label, return_counts=True)\n\n        if len(count) == 1:\n            count = np.append(count, 0)\n        val += count\n\n    print('The last values:', val)\n    return val","metadata":{"id":"D-C15yC1TeJz","execution":{"iopub.status.busy":"2024-09-04T13:46:27.459849Z","iopub.execute_input":"2024-09-04T13:46:27.460147Z","iopub.status.idle":"2024-09-04T13:46:41.424864Z","shell.execute_reply.started":"2024-09-04T13:46:27.460114Z","shell.execute_reply":"2024-09-04T13:46:41.423646Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from monai.networks.nets import UNet\nfrom monai.networks.layers import Norm\nfrom monai.losses import DiceLoss, DiceCELoss\nfrom monai.metrics import HausdorffDistanceMetric","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:46:41.429238Z","iopub.execute_input":"2024-09-04T13:46:41.429617Z","iopub.status.idle":"2024-09-04T13:46:41.512281Z","shell.execute_reply.started":"2024-09-04T13:46:41.429584Z","shell.execute_reply":"2024-09-04T13:46:41.511381Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model_dir = '/kaggle/working/results/results'","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:46:41.513419Z","iopub.execute_input":"2024-09-04T13:46:41.513705Z","iopub.status.idle":"2024-09-04T13:46:41.520171Z","shell.execute_reply.started":"2024-09-04T13:46:41.513680Z","shell.execute_reply":"2024-09-04T13:46:41.519290Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from monai.networks.nets import UNet\nfrom monai.networks.layers import Norm\nfrom monai.losses import DiceLoss, DiceCELoss\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:46:41.521457Z","iopub.execute_input":"2024-09-04T13:46:41.522431Z","iopub.status.idle":"2024-09-04T13:46:41.527441Z","shell.execute_reply.started":"2024-09-04T13:46:41.522398Z","shell.execute_reply":"2024-09-04T13:46:41.526608Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"\n\ndata_dir = '/kaggle/working/'\nmodel_dir = '/kaggle/working/results/results'\ndata_in = prepare(data_dir, cache=True)\n\ndevice = torch.device(\"cuda:0\")\n\nmodel = model = UNet(\n    spatial_dims=3,\n    in_channels=4,\n    out_channels=2,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n    norm=Norm.BATCH,\n).to(device)\n\n#loss_function = DiceCELoss(to_onehot_y=True, sigmoid=True, squared_pred=True, ce_weight=calculate_weights(1792651250,2510860).to(device))\nloss_function = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\noptimizer = torch.optim.Adam(model.parameters(), 1e-5, weight_decay=1e-5, amsgrad=True)\n\n\ntrain(model, data_in, loss_function, optimizer, 600, model_dir)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2-IZ1fMGTKWT","outputId":"e5fc03f6-24b6-4704-bfa6-23841cc37e6b","execution":{"iopub.status.busy":"2024-09-04T13:46:41.535158Z","iopub.execute_input":"2024-09-04T13:46:41.535490Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n  warn_deprecated(argname, msg, warning_category)\nLoading dataset: 100%|██████████| 400/400 [17:21<00:00,  2.60s/it]\nLoading dataset: 100%|██████████| 50/50 [02:12<00:00,  2.65s/it]\n","output_type":"stream"},{"name":"stdout","text":"----------\nepoch 1/600\n1/400, Train_loss: 0.3515\nTrain_dice: 0.6485\n2/400, Train_loss: 0.3371\nTrain_dice: 0.6629\n3/400, Train_loss: 0.3546\nTrain_dice: 0.6454\n4/400, Train_loss: 0.3329\nTrain_dice: 0.6671\n5/400, Train_loss: 0.3802\nTrain_dice: 0.6198\n6/400, Train_loss: 0.3435\nTrain_dice: 0.6565\n7/400, Train_loss: 0.3961\nTrain_dice: 0.6039\n8/400, Train_loss: 0.3442\nTrain_dice: 0.6558\n9/400, Train_loss: 0.3361\nTrain_dice: 0.6639\n10/400, Train_loss: 0.3689\nTrain_dice: 0.6311\n11/400, Train_loss: 0.3349\nTrain_dice: 0.6651\n12/400, Train_loss: 0.3357\nTrain_dice: 0.6643\n13/400, Train_loss: 0.3340\nTrain_dice: 0.6660\n14/400, Train_loss: 0.3336\nTrain_dice: 0.6664\n15/400, Train_loss: 0.3423\nTrain_dice: 0.6577\n16/400, Train_loss: 0.3411\nTrain_dice: 0.6589\n17/400, Train_loss: 0.3524\nTrain_dice: 0.6476\n18/400, Train_loss: 0.3308\nTrain_dice: 0.6692\n19/400, Train_loss: 0.4028\nTrain_dice: 0.5972\n20/400, Train_loss: 0.3395\nTrain_dice: 0.6605\n21/400, Train_loss: 0.3332\nTrain_dice: 0.6668\n22/400, Train_loss: 0.3316\nTrain_dice: 0.6684\n23/400, Train_loss: 0.3516\nTrain_dice: 0.6484\n24/400, Train_loss: 0.3313\nTrain_dice: 0.6687\n25/400, Train_loss: 0.3257\nTrain_dice: 0.6743\n26/400, Train_loss: 0.3297\nTrain_dice: 0.6703\n27/400, Train_loss: 0.3329\nTrain_dice: 0.6671\n28/400, Train_loss: 0.3324\nTrain_dice: 0.6676\n29/400, Train_loss: 0.3318\nTrain_dice: 0.6682\n30/400, Train_loss: 0.3240\nTrain_dice: 0.6760\n31/400, Train_loss: 0.4317\nTrain_dice: 0.5683\n32/400, Train_loss: 0.3529\nTrain_dice: 0.6471\n33/400, Train_loss: 0.3447\nTrain_dice: 0.6553\n34/400, Train_loss: 0.3425\nTrain_dice: 0.6575\n35/400, Train_loss: 0.3619\nTrain_dice: 0.6381\n36/400, Train_loss: 0.3247\nTrain_dice: 0.6753\n37/400, Train_loss: 0.3697\nTrain_dice: 0.6303\n38/400, Train_loss: 0.3402\nTrain_dice: 0.6598\n39/400, Train_loss: 0.3368\nTrain_dice: 0.6632\n40/400, Train_loss: 0.3254\nTrain_dice: 0.6746\n41/400, Train_loss: 0.3625\nTrain_dice: 0.6375\n42/400, Train_loss: 0.4307\nTrain_dice: 0.5693\n43/400, Train_loss: 0.3637\nTrain_dice: 0.6363\n44/400, Train_loss: 0.4050\nTrain_dice: 0.5950\n45/400, Train_loss: 0.3242\nTrain_dice: 0.6758\n46/400, Train_loss: 0.3356\nTrain_dice: 0.6644\n47/400, Train_loss: 0.3427\nTrain_dice: 0.6573\n48/400, Train_loss: 0.3457\nTrain_dice: 0.6543\n49/400, Train_loss: 0.3839\nTrain_dice: 0.6161\n50/400, Train_loss: 0.3302\nTrain_dice: 0.6698\n51/400, Train_loss: 0.3219\nTrain_dice: 0.6781\n52/400, Train_loss: 0.3356\nTrain_dice: 0.6644\n53/400, Train_loss: 0.3860\nTrain_dice: 0.6140\n54/400, Train_loss: 0.3269\nTrain_dice: 0.6731\n55/400, Train_loss: 0.3535\nTrain_dice: 0.6465\n56/400, Train_loss: 0.3355\nTrain_dice: 0.6645\n57/400, Train_loss: 0.3740\nTrain_dice: 0.6260\n58/400, Train_loss: 0.3789\nTrain_dice: 0.6211\n59/400, Train_loss: 0.3256\nTrain_dice: 0.6744\n60/400, Train_loss: 0.3371\nTrain_dice: 0.6629\n61/400, Train_loss: 0.3310\nTrain_dice: 0.6690\n62/400, Train_loss: 0.4634\nTrain_dice: 0.5366\n63/400, Train_loss: 0.3190\nTrain_dice: 0.6810\n64/400, Train_loss: 0.3265\nTrain_dice: 0.6735\n65/400, Train_loss: 0.3746\nTrain_dice: 0.6254\n66/400, Train_loss: 0.3896\nTrain_dice: 0.6104\n67/400, Train_loss: 0.3446\nTrain_dice: 0.6554\n68/400, Train_loss: 0.4061\nTrain_dice: 0.5939\n69/400, Train_loss: 0.3967\nTrain_dice: 0.6033\n70/400, Train_loss: 0.3935\nTrain_dice: 0.6065\n71/400, Train_loss: 0.3795\nTrain_dice: 0.6205\n72/400, Train_loss: 0.3664\nTrain_dice: 0.6336\n73/400, Train_loss: 0.3515\nTrain_dice: 0.6485\n74/400, Train_loss: 0.3526\nTrain_dice: 0.6474\n75/400, Train_loss: 0.3640\nTrain_dice: 0.6360\n76/400, Train_loss: 0.3302\nTrain_dice: 0.6698\n77/400, Train_loss: 0.4883\nTrain_dice: 0.5117\n78/400, Train_loss: 0.4244\nTrain_dice: 0.5756\n79/400, Train_loss: 0.3442\nTrain_dice: 0.6558\n80/400, Train_loss: 0.3612\nTrain_dice: 0.6388\n81/400, Train_loss: 0.3327\nTrain_dice: 0.6673\n82/400, Train_loss: 0.3162\nTrain_dice: 0.6838\n83/400, Train_loss: 0.3478\nTrain_dice: 0.6522\n84/400, Train_loss: 0.4916\nTrain_dice: 0.5084\n85/400, Train_loss: 0.3156\nTrain_dice: 0.6844\n86/400, Train_loss: 0.3399\nTrain_dice: 0.6601\n87/400, Train_loss: 0.4059\nTrain_dice: 0.5941\n88/400, Train_loss: 0.3195\nTrain_dice: 0.6805\n89/400, Train_loss: 0.3212\nTrain_dice: 0.6788\n90/400, Train_loss: 0.3815\nTrain_dice: 0.6185\n91/400, Train_loss: 0.3209\nTrain_dice: 0.6791\n92/400, Train_loss: 0.3543\nTrain_dice: 0.6457\n93/400, Train_loss: 0.3416\nTrain_dice: 0.6584\n94/400, Train_loss: 0.3169\nTrain_dice: 0.6831\n95/400, Train_loss: 0.4521\nTrain_dice: 0.5479\n96/400, Train_loss: 0.3358\nTrain_dice: 0.6642\n97/400, Train_loss: 0.3179\nTrain_dice: 0.6821\n98/400, Train_loss: 0.3181\nTrain_dice: 0.6819\n99/400, Train_loss: 0.3605\nTrain_dice: 0.6395\n100/400, Train_loss: 0.3736\nTrain_dice: 0.6264\n101/400, Train_loss: 0.4311\nTrain_dice: 0.5689\n102/400, Train_loss: 0.3425\nTrain_dice: 0.6575\n103/400, Train_loss: 0.3730\nTrain_dice: 0.6270\n104/400, Train_loss: 0.3729\nTrain_dice: 0.6271\n105/400, Train_loss: 0.3708\nTrain_dice: 0.6292\n106/400, Train_loss: 0.4432\nTrain_dice: 0.5568\n107/400, Train_loss: 0.4090\nTrain_dice: 0.5910\n108/400, Train_loss: 0.4065\nTrain_dice: 0.5935\n109/400, Train_loss: 0.3168\nTrain_dice: 0.6832\n110/400, Train_loss: 0.3262\nTrain_dice: 0.6738\n111/400, Train_loss: 0.3138\nTrain_dice: 0.6862\n112/400, Train_loss: 0.3130\nTrain_dice: 0.6870\n113/400, Train_loss: 0.3165\nTrain_dice: 0.6835\n114/400, Train_loss: 0.3310\nTrain_dice: 0.6690\n115/400, Train_loss: 0.3247\nTrain_dice: 0.6753\n116/400, Train_loss: 0.3164\nTrain_dice: 0.6836\n117/400, Train_loss: 0.3509\nTrain_dice: 0.6491\n118/400, Train_loss: 0.3294\nTrain_dice: 0.6706\n119/400, Train_loss: 0.3838\nTrain_dice: 0.6162\n120/400, Train_loss: 0.3422\nTrain_dice: 0.6578\n121/400, Train_loss: 0.3353\nTrain_dice: 0.6647\n122/400, Train_loss: 0.3497\nTrain_dice: 0.6503\n123/400, Train_loss: 0.4378\nTrain_dice: 0.5622\n124/400, Train_loss: 0.3855\nTrain_dice: 0.6145\n125/400, Train_loss: 0.4174\nTrain_dice: 0.5826\n126/400, Train_loss: 0.3373\nTrain_dice: 0.6627\n127/400, Train_loss: 0.3168\nTrain_dice: 0.6832\n128/400, Train_loss: 0.3190\nTrain_dice: 0.6810\n129/400, Train_loss: 0.3233\nTrain_dice: 0.6767\n130/400, Train_loss: 0.3383\nTrain_dice: 0.6617\n131/400, Train_loss: 0.3363\nTrain_dice: 0.6637\n132/400, Train_loss: 0.3209\nTrain_dice: 0.6791\n133/400, Train_loss: 0.3331\nTrain_dice: 0.6669\n134/400, Train_loss: 0.3824\nTrain_dice: 0.6176\n135/400, Train_loss: 0.3410\nTrain_dice: 0.6590\n136/400, Train_loss: 0.3824\nTrain_dice: 0.6176\n137/400, Train_loss: 0.3925\nTrain_dice: 0.6075\n138/400, Train_loss: 0.4909\nTrain_dice: 0.5091\n139/400, Train_loss: 0.3846\nTrain_dice: 0.6154\n140/400, Train_loss: 0.3533\nTrain_dice: 0.6467\n141/400, Train_loss: 0.3403\nTrain_dice: 0.6597\n142/400, Train_loss: 0.3196\nTrain_dice: 0.6804\n143/400, Train_loss: 0.3103\nTrain_dice: 0.6897\n144/400, Train_loss: 0.3869\nTrain_dice: 0.6131\n145/400, Train_loss: 0.3105\nTrain_dice: 0.6895\n146/400, Train_loss: 0.3190\nTrain_dice: 0.6810\n147/400, Train_loss: 0.3799\nTrain_dice: 0.6201\n148/400, Train_loss: 0.3500\nTrain_dice: 0.6500\n149/400, Train_loss: 0.3773\nTrain_dice: 0.6227\n150/400, Train_loss: 0.3690\nTrain_dice: 0.6310\n151/400, Train_loss: 0.3754\nTrain_dice: 0.6246\n152/400, Train_loss: 0.3726\nTrain_dice: 0.6274\n153/400, Train_loss: 0.3608\nTrain_dice: 0.6392\n154/400, Train_loss: 0.4217\nTrain_dice: 0.5783\n155/400, Train_loss: 0.3337\nTrain_dice: 0.6663\n156/400, Train_loss: 0.3830\nTrain_dice: 0.6170\n157/400, Train_loss: 0.4094\nTrain_dice: 0.5906\n158/400, Train_loss: 0.3223\nTrain_dice: 0.6777\n159/400, Train_loss: 0.3279\nTrain_dice: 0.6721\n160/400, Train_loss: 0.4405\nTrain_dice: 0.5595\n161/400, Train_loss: 0.3462\nTrain_dice: 0.6538\n162/400, Train_loss: 0.3177\nTrain_dice: 0.6823\n163/400, Train_loss: 0.3776\nTrain_dice: 0.6224\n164/400, Train_loss: 0.3253\nTrain_dice: 0.6747\n165/400, Train_loss: 0.3376\nTrain_dice: 0.6624\n166/400, Train_loss: 0.2970\nTrain_dice: 0.7030\n167/400, Train_loss: 0.3115\nTrain_dice: 0.6885\n168/400, Train_loss: 0.3644\nTrain_dice: 0.6356\n169/400, Train_loss: 0.3309\nTrain_dice: 0.6691\n170/400, Train_loss: 0.3290\nTrain_dice: 0.6710\n171/400, Train_loss: 0.3179\nTrain_dice: 0.6821\n172/400, Train_loss: 0.3016\nTrain_dice: 0.6984\n173/400, Train_loss: 0.3014\nTrain_dice: 0.6986\n174/400, Train_loss: 0.3223\nTrain_dice: 0.6777\n175/400, Train_loss: 0.4012\nTrain_dice: 0.5988\n176/400, Train_loss: 0.3905\nTrain_dice: 0.6095\n177/400, Train_loss: 0.4103\nTrain_dice: 0.5897\n178/400, Train_loss: 0.5158\nTrain_dice: 0.4842\n179/400, Train_loss: 0.4579\nTrain_dice: 0.5421\n180/400, Train_loss: 0.3755\nTrain_dice: 0.6245\n181/400, Train_loss: 0.3086\nTrain_dice: 0.6914\n182/400, Train_loss: 0.2976\nTrain_dice: 0.7024\n183/400, Train_loss: 0.3645\nTrain_dice: 0.6355\n184/400, Train_loss: 0.3983\nTrain_dice: 0.6017\n185/400, Train_loss: 0.3691\nTrain_dice: 0.6309\n186/400, Train_loss: 0.4133\nTrain_dice: 0.5867\n187/400, Train_loss: 0.3871\nTrain_dice: 0.6129\n188/400, Train_loss: 0.3681\nTrain_dice: 0.6319\n189/400, Train_loss: 0.3943\nTrain_dice: 0.6057\n190/400, Train_loss: 0.3195\nTrain_dice: 0.6805\n191/400, Train_loss: 0.3119\nTrain_dice: 0.6881\n192/400, Train_loss: 0.3948\nTrain_dice: 0.6052\n193/400, Train_loss: 0.3136\nTrain_dice: 0.6864\n194/400, Train_loss: 0.3744\nTrain_dice: 0.6256\n195/400, Train_loss: 0.3223\nTrain_dice: 0.6777\n196/400, Train_loss: 0.3233\nTrain_dice: 0.6767\n197/400, Train_loss: 0.4162\nTrain_dice: 0.5838\n198/400, Train_loss: 0.3064\nTrain_dice: 0.6936\n199/400, Train_loss: 0.3023\nTrain_dice: 0.6977\n200/400, Train_loss: 0.3098\nTrain_dice: 0.6902\n201/400, Train_loss: 0.3338\nTrain_dice: 0.6662\n202/400, Train_loss: 0.3083\nTrain_dice: 0.6917\n203/400, Train_loss: 0.3632\nTrain_dice: 0.6368\n204/400, Train_loss: 0.3411\nTrain_dice: 0.6589\n205/400, Train_loss: 0.3209\nTrain_dice: 0.6791\n206/400, Train_loss: 0.3139\nTrain_dice: 0.6861\n207/400, Train_loss: 0.3369\nTrain_dice: 0.6631\n208/400, Train_loss: 0.3089\nTrain_dice: 0.6911\n209/400, Train_loss: 0.3284\nTrain_dice: 0.6716\n210/400, Train_loss: 0.3254\nTrain_dice: 0.6746\n211/400, Train_loss: 0.3860\nTrain_dice: 0.6140\n212/400, Train_loss: 0.4343\nTrain_dice: 0.5657\n213/400, Train_loss: 0.4139\nTrain_dice: 0.5861\n214/400, Train_loss: 0.3856\nTrain_dice: 0.6144\n215/400, Train_loss: 0.3994\nTrain_dice: 0.6006\n216/400, Train_loss: 0.4132\nTrain_dice: 0.5868\n217/400, Train_loss: 0.3090\nTrain_dice: 0.6910\n218/400, Train_loss: 0.2903\nTrain_dice: 0.7097\n219/400, Train_loss: 0.3462\nTrain_dice: 0.6538\n220/400, Train_loss: 0.3262\nTrain_dice: 0.6738\n221/400, Train_loss: 0.3057\nTrain_dice: 0.6943\n222/400, Train_loss: 0.3626\nTrain_dice: 0.6374\n223/400, Train_loss: 0.3490\nTrain_dice: 0.6510\n224/400, Train_loss: 0.3943\nTrain_dice: 0.6057\n225/400, Train_loss: 0.3001\nTrain_dice: 0.6999\n226/400, Train_loss: 0.3962\nTrain_dice: 0.6038\n227/400, Train_loss: 0.3342\nTrain_dice: 0.6658\n228/400, Train_loss: 0.3411\nTrain_dice: 0.6589\n229/400, Train_loss: 0.2914\nTrain_dice: 0.7086\n230/400, Train_loss: 0.3080\nTrain_dice: 0.6920\n231/400, Train_loss: 0.2938\nTrain_dice: 0.7062\n232/400, Train_loss: 0.3135\nTrain_dice: 0.6865\n233/400, Train_loss: 0.4694\nTrain_dice: 0.5306\n234/400, Train_loss: 0.3363\nTrain_dice: 0.6637\n235/400, Train_loss: 0.2955\nTrain_dice: 0.7045\n236/400, Train_loss: 0.3224\nTrain_dice: 0.6776\n237/400, Train_loss: 0.2961\nTrain_dice: 0.7039\n238/400, Train_loss: 0.4759\nTrain_dice: 0.5241\n239/400, Train_loss: 0.3116\nTrain_dice: 0.6884\n240/400, Train_loss: 0.3890\nTrain_dice: 0.6110\n241/400, Train_loss: 0.3540\nTrain_dice: 0.6460\n242/400, Train_loss: 0.3440\nTrain_dice: 0.6560\n243/400, Train_loss: 0.2997\nTrain_dice: 0.7003\n244/400, Train_loss: 0.2840\nTrain_dice: 0.7160\n245/400, Train_loss: 0.3655\nTrain_dice: 0.6345\n246/400, Train_loss: 0.3561\nTrain_dice: 0.6439\n247/400, Train_loss: 0.3619\nTrain_dice: 0.6381\n248/400, Train_loss: 0.4167\nTrain_dice: 0.5833\n249/400, Train_loss: 0.2839\nTrain_dice: 0.7161\n250/400, Train_loss: 0.3375\nTrain_dice: 0.6625\n251/400, Train_loss: 0.3245\nTrain_dice: 0.6755\n252/400, Train_loss: 0.3156\nTrain_dice: 0.6844\n253/400, Train_loss: 0.4393\nTrain_dice: 0.5607\n254/400, Train_loss: 0.3927\nTrain_dice: 0.6073\n255/400, Train_loss: 0.2809\nTrain_dice: 0.7191\n256/400, Train_loss: 0.2957\nTrain_dice: 0.7043\n257/400, Train_loss: 0.3806\nTrain_dice: 0.6194\n258/400, Train_loss: 0.3645\nTrain_dice: 0.6355\n259/400, Train_loss: 0.2840\nTrain_dice: 0.7160\n260/400, Train_loss: 0.2740\nTrain_dice: 0.7260\n261/400, Train_loss: 0.2707\nTrain_dice: 0.7293\n262/400, Train_loss: 0.2795\nTrain_dice: 0.7205\n263/400, Train_loss: 0.2895\nTrain_dice: 0.7105\n264/400, Train_loss: 0.3069\nTrain_dice: 0.6931\n265/400, Train_loss: 0.2757\nTrain_dice: 0.7243\n266/400, Train_loss: 0.2764\nTrain_dice: 0.7236\n267/400, Train_loss: 0.2939\nTrain_dice: 0.7061\n268/400, Train_loss: 0.3587\nTrain_dice: 0.6413\n269/400, Train_loss: 0.3039\nTrain_dice: 0.6961\n270/400, Train_loss: 0.3315\nTrain_dice: 0.6685\n271/400, Train_loss: 0.3750\nTrain_dice: 0.6250\n272/400, Train_loss: 0.3246\nTrain_dice: 0.6754\n273/400, Train_loss: 0.3324\nTrain_dice: 0.6676\n274/400, Train_loss: 0.2871\nTrain_dice: 0.7129\n275/400, Train_loss: 0.3020\nTrain_dice: 0.6980\n276/400, Train_loss: 0.3021\nTrain_dice: 0.6979\n277/400, Train_loss: 0.2875\nTrain_dice: 0.7125\n278/400, Train_loss: 0.3426\nTrain_dice: 0.6574\n279/400, Train_loss: 0.2930\nTrain_dice: 0.7070\n280/400, Train_loss: 0.2917\nTrain_dice: 0.7083\n281/400, Train_loss: 0.3056\nTrain_dice: 0.6944\n282/400, Train_loss: 0.3363\nTrain_dice: 0.6637\n283/400, Train_loss: 0.2887\nTrain_dice: 0.7113\n284/400, Train_loss: 0.2782\nTrain_dice: 0.7218\n285/400, Train_loss: 0.2899\nTrain_dice: 0.7101\n286/400, Train_loss: 0.2827\nTrain_dice: 0.7173\n287/400, Train_loss: 0.2872\nTrain_dice: 0.7128\n288/400, Train_loss: 0.2987\nTrain_dice: 0.7013\n289/400, Train_loss: 0.2967\nTrain_dice: 0.7033\n290/400, Train_loss: 0.3710\nTrain_dice: 0.6290\n291/400, Train_loss: 0.3188\nTrain_dice: 0.6812\n292/400, Train_loss: 0.3650\nTrain_dice: 0.6350\n293/400, Train_loss: 0.2856\nTrain_dice: 0.7144\n294/400, Train_loss: 0.2803\nTrain_dice: 0.7197\n295/400, Train_loss: 0.3140\nTrain_dice: 0.6860\n296/400, Train_loss: 0.3200\nTrain_dice: 0.6800\n297/400, Train_loss: 0.3697\nTrain_dice: 0.6303\n298/400, Train_loss: 0.2711\nTrain_dice: 0.7289\n299/400, Train_loss: 0.3737\nTrain_dice: 0.6263\n300/400, Train_loss: 0.3013\nTrain_dice: 0.6987\n301/400, Train_loss: 0.3293\nTrain_dice: 0.6707\n302/400, Train_loss: 0.3532\nTrain_dice: 0.6468\n303/400, Train_loss: 0.3401\nTrain_dice: 0.6599\n304/400, Train_loss: 0.3438\nTrain_dice: 0.6562\n305/400, Train_loss: 0.3557\nTrain_dice: 0.6443\n306/400, Train_loss: 0.3471\nTrain_dice: 0.6529\n307/400, Train_loss: 0.3783\nTrain_dice: 0.6217\n308/400, Train_loss: 0.3054\nTrain_dice: 0.6946\n309/400, Train_loss: 0.2920\nTrain_dice: 0.7080\n310/400, Train_loss: 0.2906\nTrain_dice: 0.7094\n311/400, Train_loss: 0.3044\nTrain_dice: 0.6956\n312/400, Train_loss: 0.3876\nTrain_dice: 0.6124\n313/400, Train_loss: 0.2765\nTrain_dice: 0.7235\n314/400, Train_loss: 0.3060\nTrain_dice: 0.6940\n315/400, Train_loss: 0.3823\nTrain_dice: 0.6177\n316/400, Train_loss: 0.3328\nTrain_dice: 0.6672\n317/400, Train_loss: 0.4069\nTrain_dice: 0.5931\n318/400, Train_loss: 0.3312\nTrain_dice: 0.6688\n319/400, Train_loss: 0.3568\nTrain_dice: 0.6432\n320/400, Train_loss: 0.2918\nTrain_dice: 0.7082\n321/400, Train_loss: 0.3565\nTrain_dice: 0.6435\n322/400, Train_loss: 0.3458\nTrain_dice: 0.6542\n323/400, Train_loss: 0.3235\nTrain_dice: 0.6765\n324/400, Train_loss: 0.3591\nTrain_dice: 0.6409\n325/400, Train_loss: 0.3639\nTrain_dice: 0.6361\n326/400, Train_loss: 0.2958\nTrain_dice: 0.7042\n327/400, Train_loss: 0.2504\nTrain_dice: 0.7496\n328/400, Train_loss: 0.2712\nTrain_dice: 0.7288\n329/400, Train_loss: 0.3843\nTrain_dice: 0.6157\n330/400, Train_loss: 0.3274\nTrain_dice: 0.6726\n331/400, Train_loss: 0.2894\nTrain_dice: 0.7106\n332/400, Train_loss: 0.3178\nTrain_dice: 0.6822\n333/400, Train_loss: 0.2952\nTrain_dice: 0.7048\n334/400, Train_loss: 0.2940\nTrain_dice: 0.7060\n335/400, Train_loss: 0.3125\nTrain_dice: 0.6875\n336/400, Train_loss: 0.2853\nTrain_dice: 0.7147\n337/400, Train_loss: 0.3865\nTrain_dice: 0.6135\n338/400, Train_loss: 0.4292\nTrain_dice: 0.5708\n339/400, Train_loss: 0.3084\nTrain_dice: 0.6916\n340/400, Train_loss: 0.3675\nTrain_dice: 0.6325\n341/400, Train_loss: 0.3224\nTrain_dice: 0.6776\n342/400, Train_loss: 0.3681\nTrain_dice: 0.6319\n343/400, Train_loss: 0.2849\nTrain_dice: 0.7151\n344/400, Train_loss: 0.3254\nTrain_dice: 0.6746\n345/400, Train_loss: 0.3016\nTrain_dice: 0.6984\n346/400, Train_loss: 0.2970\nTrain_dice: 0.7030\n347/400, Train_loss: 0.2769\nTrain_dice: 0.7231\n348/400, Train_loss: 0.3396\nTrain_dice: 0.6604\n349/400, Train_loss: 0.2742\nTrain_dice: 0.7258\n350/400, Train_loss: 0.3816\nTrain_dice: 0.6184\n351/400, Train_loss: 0.2723\nTrain_dice: 0.7277\n352/400, Train_loss: 0.3374\nTrain_dice: 0.6626\n353/400, Train_loss: 0.3593\nTrain_dice: 0.6407\n354/400, Train_loss: 0.2945\nTrain_dice: 0.7055\n355/400, Train_loss: 0.3696\nTrain_dice: 0.6304\n356/400, Train_loss: 0.2988\nTrain_dice: 0.7012\n357/400, Train_loss: 0.3178\nTrain_dice: 0.6822\n358/400, Train_loss: 0.3390\nTrain_dice: 0.6610\n359/400, Train_loss: 0.3730\nTrain_dice: 0.6270\n360/400, Train_loss: 0.3572\nTrain_dice: 0.6428\n361/400, Train_loss: 0.2935\nTrain_dice: 0.7065\n362/400, Train_loss: 0.2986\nTrain_dice: 0.7014\n363/400, Train_loss: 0.3830\nTrain_dice: 0.6170\n364/400, Train_loss: 0.3995\nTrain_dice: 0.6005\n365/400, Train_loss: 0.3005\nTrain_dice: 0.6995\n366/400, Train_loss: 0.3469\nTrain_dice: 0.6531\n367/400, Train_loss: 0.2981\nTrain_dice: 0.7019\n368/400, Train_loss: 0.3596\nTrain_dice: 0.6404\n369/400, Train_loss: 0.2774\nTrain_dice: 0.7226\n370/400, Train_loss: 0.3077\nTrain_dice: 0.6923\n371/400, Train_loss: 0.3040\nTrain_dice: 0.6960\n372/400, Train_loss: 0.3807\nTrain_dice: 0.6193\n373/400, Train_loss: 0.2920\nTrain_dice: 0.7080\n374/400, Train_loss: 0.3123\nTrain_dice: 0.6877\n375/400, Train_loss: 0.2940\nTrain_dice: 0.7060\n376/400, Train_loss: 0.3524\nTrain_dice: 0.6476\n377/400, Train_loss: 0.2915\nTrain_dice: 0.7085\n378/400, Train_loss: 0.3190\nTrain_dice: 0.6810\n379/400, Train_loss: 0.2822\nTrain_dice: 0.7178\n380/400, Train_loss: 0.3424\nTrain_dice: 0.6576\n381/400, Train_loss: 0.3081\nTrain_dice: 0.6919\n382/400, Train_loss: 0.3540\nTrain_dice: 0.6460\n383/400, Train_loss: 0.3086\nTrain_dice: 0.6914\n384/400, Train_loss: 0.2929\nTrain_dice: 0.7071\n385/400, Train_loss: 0.2437\nTrain_dice: 0.7563\n386/400, Train_loss: 0.3609\nTrain_dice: 0.6391\n387/400, Train_loss: 0.3211\nTrain_dice: 0.6789\n388/400, Train_loss: 0.2444\nTrain_dice: 0.7556\n389/400, Train_loss: 0.3490\nTrain_dice: 0.6510\n390/400, Train_loss: 0.2810\nTrain_dice: 0.7190\n391/400, Train_loss: 0.2615\nTrain_dice: 0.7385\n392/400, Train_loss: 0.2699\nTrain_dice: 0.7301\n393/400, Train_loss: 0.3758\nTrain_dice: 0.6242\n394/400, Train_loss: 0.2896\nTrain_dice: 0.7104\n395/400, Train_loss: 0.2631\nTrain_dice: 0.7369\n396/400, Train_loss: 0.2562\nTrain_dice: 0.7438\n397/400, Train_loss: 0.2532\nTrain_dice: 0.7468\n398/400, Train_loss: 0.2575\nTrain_dice: 0.7425\n399/400, Train_loss: 0.2707\nTrain_dice: 0.7293\n400/400, Train_loss: 0.2618\nTrain_dice: 0.7382\n--------------------\nEpoch_loss: 0.3406\nEpoch_metric: 0.6594\ntest_loss_epoch: 0.4609\ntest_dice_epoch: 0.5391\ncurrent epoch: 1 current mean dice: 0.5335\nbest mean dice: 0.5391 at epoch: 1\n----------\nepoch 2/600\n1/400, Train_loss: 0.2787\nTrain_dice: 0.7213\n2/400, Train_loss: 0.2602\nTrain_dice: 0.7398\n3/400, Train_loss: 0.2832\nTrain_dice: 0.7168\n4/400, Train_loss: 0.2488\nTrain_dice: 0.7512\n5/400, Train_loss: 0.3225\nTrain_dice: 0.6775\n6/400, Train_loss: 0.2598\nTrain_dice: 0.7402\n7/400, Train_loss: 0.3576\nTrain_dice: 0.6424\n8/400, Train_loss: 0.2686\nTrain_dice: 0.7314\n9/400, Train_loss: 0.2573\nTrain_dice: 0.7427\n10/400, Train_loss: 0.3259\nTrain_dice: 0.6741\n11/400, Train_loss: 0.2549\nTrain_dice: 0.7451\n12/400, Train_loss: 0.2665\nTrain_dice: 0.7335\n13/400, Train_loss: 0.2745\nTrain_dice: 0.7255\n14/400, Train_loss: 0.2542\nTrain_dice: 0.7458\n15/400, Train_loss: 0.2637\nTrain_dice: 0.7363\n16/400, Train_loss: 0.2680\nTrain_dice: 0.7320\n17/400, Train_loss: 0.2944\nTrain_dice: 0.7056\n18/400, Train_loss: 0.2672\nTrain_dice: 0.7328\n19/400, Train_loss: 0.3503\nTrain_dice: 0.6497\n20/400, Train_loss: 0.2595\nTrain_dice: 0.7405\n21/400, Train_loss: 0.2671\nTrain_dice: 0.7329\n22/400, Train_loss: 0.2390\nTrain_dice: 0.7610\n23/400, Train_loss: 0.2979\nTrain_dice: 0.7021\n24/400, Train_loss: 0.2549\nTrain_dice: 0.7451\n25/400, Train_loss: 0.2408\nTrain_dice: 0.7592\n26/400, Train_loss: 0.2599\nTrain_dice: 0.7401\n27/400, Train_loss: 0.2551\nTrain_dice: 0.7449\n28/400, Train_loss: 0.2543\nTrain_dice: 0.7457\n29/400, Train_loss: 0.2758\nTrain_dice: 0.7242\n30/400, Train_loss: 0.2350\nTrain_dice: 0.7650\n31/400, Train_loss: 0.3992\nTrain_dice: 0.6008\n32/400, Train_loss: 0.2909\nTrain_dice: 0.7091\n33/400, Train_loss: 0.2763\nTrain_dice: 0.7237\n34/400, Train_loss: 0.2736\nTrain_dice: 0.7264\n35/400, Train_loss: 0.3070\nTrain_dice: 0.6930\n36/400, Train_loss: 0.2258\nTrain_dice: 0.7742\n37/400, Train_loss: 0.3278\nTrain_dice: 0.6722\n38/400, Train_loss: 0.2770\nTrain_dice: 0.7230\n39/400, Train_loss: 0.2665\nTrain_dice: 0.7335\n40/400, Train_loss: 0.2371\nTrain_dice: 0.7629\n41/400, Train_loss: 0.3057\nTrain_dice: 0.6943\n42/400, Train_loss: 0.4056\nTrain_dice: 0.5944\n43/400, Train_loss: 0.3194\nTrain_dice: 0.6806\n44/400, Train_loss: 0.3616\nTrain_dice: 0.6384\n45/400, Train_loss: 0.2356\nTrain_dice: 0.7644\n46/400, Train_loss: 0.2574\nTrain_dice: 0.7426\n47/400, Train_loss: 0.2687\nTrain_dice: 0.7313\n48/400, Train_loss: 0.2760\nTrain_dice: 0.7240\n49/400, Train_loss: 0.3424\nTrain_dice: 0.6576\n50/400, Train_loss: 0.2522\nTrain_dice: 0.7478\n51/400, Train_loss: 0.2355\nTrain_dice: 0.7645\n52/400, Train_loss: 0.2659\nTrain_dice: 0.7341\n53/400, Train_loss: 0.3438\nTrain_dice: 0.6562\n54/400, Train_loss: 0.2464\nTrain_dice: 0.7536\n55/400, Train_loss: 0.3011\nTrain_dice: 0.6989\n56/400, Train_loss: 0.2651\nTrain_dice: 0.7349\n57/400, Train_loss: 0.3291\nTrain_dice: 0.6709\n58/400, Train_loss: 0.3310\nTrain_dice: 0.6690\n59/400, Train_loss: 0.2480\nTrain_dice: 0.7520\n60/400, Train_loss: 0.2636\nTrain_dice: 0.7364\n61/400, Train_loss: 0.2536\nTrain_dice: 0.7464\n62/400, Train_loss: 0.4480\nTrain_dice: 0.5520\n63/400, Train_loss: 0.2364\nTrain_dice: 0.7636\n64/400, Train_loss: 0.2480\nTrain_dice: 0.7520\n65/400, Train_loss: 0.3248\nTrain_dice: 0.6752\n66/400, Train_loss: 0.3417\nTrain_dice: 0.6583\n67/400, Train_loss: 0.2767\nTrain_dice: 0.7233\n68/400, Train_loss: 0.3734\nTrain_dice: 0.6266\n69/400, Train_loss: 0.3603\nTrain_dice: 0.6397\n70/400, Train_loss: 0.3539\nTrain_dice: 0.6461\n71/400, Train_loss: 0.3398\nTrain_dice: 0.6602\n72/400, Train_loss: 0.3209\nTrain_dice: 0.6791\n73/400, Train_loss: 0.3014\nTrain_dice: 0.6986\n74/400, Train_loss: 0.2962\nTrain_dice: 0.7038\n75/400, Train_loss: 0.3102\nTrain_dice: 0.6898\n76/400, Train_loss: 0.2547\nTrain_dice: 0.7453\n77/400, Train_loss: 0.4798\nTrain_dice: 0.5202\n78/400, Train_loss: 0.3997\nTrain_dice: 0.6003\n79/400, Train_loss: 0.2946\nTrain_dice: 0.7054\n80/400, Train_loss: 0.3180\nTrain_dice: 0.6820\n81/400, Train_loss: 0.2665\nTrain_dice: 0.7335\n82/400, Train_loss: 0.2264\nTrain_dice: 0.7736\n83/400, Train_loss: 0.2971\nTrain_dice: 0.7029\n84/400, Train_loss: 0.4823\nTrain_dice: 0.5177\n85/400, Train_loss: 0.2321\nTrain_dice: 0.7679\n86/400, Train_loss: 0.2831\nTrain_dice: 0.7169\n87/400, Train_loss: 0.3765\nTrain_dice: 0.6235\n88/400, Train_loss: 0.2426\nTrain_dice: 0.7574\n89/400, Train_loss: 0.2396\nTrain_dice: 0.7604\n90/400, Train_loss: 0.3399\nTrain_dice: 0.6601\n91/400, Train_loss: 0.2389\nTrain_dice: 0.7611\n92/400, Train_loss: 0.3091\nTrain_dice: 0.6909\n93/400, Train_loss: 0.2802\nTrain_dice: 0.7198\n94/400, Train_loss: 0.2370\nTrain_dice: 0.7630\n95/400, Train_loss: 0.4338\nTrain_dice: 0.5662\n96/400, Train_loss: 0.2657\nTrain_dice: 0.7343\n97/400, Train_loss: 0.2368\nTrain_dice: 0.7632\n98/400, Train_loss: 0.2434\nTrain_dice: 0.7566\n99/400, Train_loss: 0.3198\nTrain_dice: 0.6802\n100/400, Train_loss: 0.3395\nTrain_dice: 0.6605\n101/400, Train_loss: 0.4163\nTrain_dice: 0.5837\n102/400, Train_loss: 0.2816\nTrain_dice: 0.7184\n103/400, Train_loss: 0.3339\nTrain_dice: 0.6661\n104/400, Train_loss: 0.3304\nTrain_dice: 0.6696\n105/400, Train_loss: 0.3283\nTrain_dice: 0.6717\n106/400, Train_loss: 0.4205\nTrain_dice: 0.5795\n107/400, Train_loss: 0.3763\nTrain_dice: 0.6237\n108/400, Train_loss: 0.3745\nTrain_dice: 0.6255\n109/400, Train_loss: 0.2393\nTrain_dice: 0.7607\n110/400, Train_loss: 0.2565\nTrain_dice: 0.7435\n111/400, Train_loss: 0.2355\nTrain_dice: 0.7645\n112/400, Train_loss: 0.2346\nTrain_dice: 0.7654\n113/400, Train_loss: 0.2427\nTrain_dice: 0.7573\n114/400, Train_loss: 0.2787\nTrain_dice: 0.7213\n115/400, Train_loss: 0.2586\nTrain_dice: 0.7414\n116/400, Train_loss: 0.2470\nTrain_dice: 0.7530\n117/400, Train_loss: 0.3027\nTrain_dice: 0.6973\n118/400, Train_loss: 0.2733\nTrain_dice: 0.7267\n119/400, Train_loss: 0.3511\nTrain_dice: 0.6489\n120/400, Train_loss: 0.2942\nTrain_dice: 0.7058\n121/400, Train_loss: 0.2855\nTrain_dice: 0.7145\n122/400, Train_loss: 0.2963\nTrain_dice: 0.7037\n123/400, Train_loss: 0.4147\nTrain_dice: 0.5853\n124/400, Train_loss: 0.3406\nTrain_dice: 0.6594\n125/400, Train_loss: 0.3796\nTrain_dice: 0.6204\n126/400, Train_loss: 0.2863\nTrain_dice: 0.7137\n127/400, Train_loss: 0.2432\nTrain_dice: 0.7568\n128/400, Train_loss: 0.2608\nTrain_dice: 0.7392\n129/400, Train_loss: 0.2572\nTrain_dice: 0.7428\n130/400, Train_loss: 0.2926\nTrain_dice: 0.7074\n131/400, Train_loss: 0.2906\nTrain_dice: 0.7094\n132/400, Train_loss: 0.2635\nTrain_dice: 0.7365\n133/400, Train_loss: 0.2785\nTrain_dice: 0.7215\n134/400, Train_loss: 0.3508\nTrain_dice: 0.6492\n135/400, Train_loss: 0.2948\nTrain_dice: 0.7052\n136/400, Train_loss: 0.3547\nTrain_dice: 0.6453\n137/400, Train_loss: 0.3693\nTrain_dice: 0.6307\n138/400, Train_loss: 0.4846\nTrain_dice: 0.5154\n139/400, Train_loss: 0.3561\nTrain_dice: 0.6439\n140/400, Train_loss: 0.3132\nTrain_dice: 0.6868\n141/400, Train_loss: 0.2912\nTrain_dice: 0.7088\n142/400, Train_loss: 0.2508\nTrain_dice: 0.7492\n143/400, Train_loss: 0.2377\nTrain_dice: 0.7623\n144/400, Train_loss: 0.3486\nTrain_dice: 0.6514\n145/400, Train_loss: 0.2394\nTrain_dice: 0.7606\n146/400, Train_loss: 0.2595\nTrain_dice: 0.7405\n147/400, Train_loss: 0.3411\nTrain_dice: 0.6589\n148/400, Train_loss: 0.3011\nTrain_dice: 0.6989\n149/400, Train_loss: 0.3355\nTrain_dice: 0.6645\n150/400, Train_loss: 0.3246\nTrain_dice: 0.6754\n151/400, Train_loss: 0.3351\nTrain_dice: 0.6649\n152/400, Train_loss: 0.3295\nTrain_dice: 0.6705\n153/400, Train_loss: 0.3212\nTrain_dice: 0.6788\n154/400, Train_loss: 0.4068\nTrain_dice: 0.5932\n155/400, Train_loss: 0.2707\nTrain_dice: 0.7293\n156/400, Train_loss: 0.3524\nTrain_dice: 0.6476\n157/400, Train_loss: 0.3843\nTrain_dice: 0.6157\n158/400, Train_loss: 0.2613\nTrain_dice: 0.7387\n159/400, Train_loss: 0.2706\nTrain_dice: 0.7294\n160/400, Train_loss: 0.4228\nTrain_dice: 0.5772\n161/400, Train_loss: 0.3063\nTrain_dice: 0.6937\n162/400, Train_loss: 0.2569\nTrain_dice: 0.7431\n163/400, Train_loss: 0.3483\nTrain_dice: 0.6517\n164/400, Train_loss: 0.2723\nTrain_dice: 0.7277\n165/400, Train_loss: 0.2833\nTrain_dice: 0.7167\n166/400, Train_loss: 0.2174\nTrain_dice: 0.7826\n167/400, Train_loss: 0.2463\nTrain_dice: 0.7537\n168/400, Train_loss: 0.3282\nTrain_dice: 0.6718\n169/400, Train_loss: 0.2882\nTrain_dice: 0.7118\n170/400, Train_loss: 0.2817\nTrain_dice: 0.7183\n171/400, Train_loss: 0.2636\nTrain_dice: 0.7364\n172/400, Train_loss: 0.2274\nTrain_dice: 0.7726\n173/400, Train_loss: 0.2279\nTrain_dice: 0.7721\n174/400, Train_loss: 0.2589\nTrain_dice: 0.7411\n175/400, Train_loss: 0.3735\nTrain_dice: 0.6265\n176/400, Train_loss: 0.3601\nTrain_dice: 0.6399\n177/400, Train_loss: 0.3866\nTrain_dice: 0.6134\n178/400, Train_loss: 0.5142\nTrain_dice: 0.4858\n179/400, Train_loss: 0.4468\nTrain_dice: 0.5532\n180/400, Train_loss: 0.3478\nTrain_dice: 0.6522\n181/400, Train_loss: 0.2431\nTrain_dice: 0.7569\n182/400, Train_loss: 0.2274\nTrain_dice: 0.7726\n183/400, Train_loss: 0.3231\nTrain_dice: 0.6769\n184/400, Train_loss: 0.3663\nTrain_dice: 0.6337\n185/400, Train_loss: 0.3333\nTrain_dice: 0.6667\n186/400, Train_loss: 0.3862\nTrain_dice: 0.6138\n187/400, Train_loss: 0.3543\nTrain_dice: 0.6457\n188/400, Train_loss: 0.3308\nTrain_dice: 0.6692\n189/400, Train_loss: 0.3638\nTrain_dice: 0.6362\n190/400, Train_loss: 0.2558\nTrain_dice: 0.7442\n191/400, Train_loss: 0.2486\nTrain_dice: 0.7514\n192/400, Train_loss: 0.3713\nTrain_dice: 0.6287\n193/400, Train_loss: 0.2517\nTrain_dice: 0.7483\n194/400, Train_loss: 0.3440\nTrain_dice: 0.6560\n195/400, Train_loss: 0.2754\nTrain_dice: 0.7246\n196/400, Train_loss: 0.2656\nTrain_dice: 0.7344\n197/400, Train_loss: 0.3944\nTrain_dice: 0.6056\n198/400, Train_loss: 0.2422\nTrain_dice: 0.7578\n199/400, Train_loss: 0.2485\nTrain_dice: 0.7515\n200/400, Train_loss: 0.2457\nTrain_dice: 0.7543\n201/400, Train_loss: 0.2875\nTrain_dice: 0.7125\n202/400, Train_loss: 0.2469\nTrain_dice: 0.7531\n203/400, Train_loss: 0.3229\nTrain_dice: 0.6771\n204/400, Train_loss: 0.2996\nTrain_dice: 0.7004\n205/400, Train_loss: 0.2684\nTrain_dice: 0.7316\n206/400, Train_loss: 0.2603\nTrain_dice: 0.7397\n207/400, Train_loss: 0.2941\nTrain_dice: 0.7059\n208/400, Train_loss: 0.2558\nTrain_dice: 0.7442\n209/400, Train_loss: 0.2891\nTrain_dice: 0.7109\n210/400, Train_loss: 0.2800\nTrain_dice: 0.7200\n211/400, Train_loss: 0.3608\nTrain_dice: 0.6392\n212/400, Train_loss: 0.4183\nTrain_dice: 0.5817\n213/400, Train_loss: 0.3967\nTrain_dice: 0.6033\n214/400, Train_loss: 0.3590\nTrain_dice: 0.6410\n215/400, Train_loss: 0.3769\nTrain_dice: 0.6231\n216/400, Train_loss: 0.3914\nTrain_dice: 0.6086\n217/400, Train_loss: 0.2474\nTrain_dice: 0.7526\n218/400, Train_loss: 0.2323\nTrain_dice: 0.7677\n219/400, Train_loss: 0.3002\nTrain_dice: 0.6998\n220/400, Train_loss: 0.2802\nTrain_dice: 0.7198\n221/400, Train_loss: 0.2560\nTrain_dice: 0.7440\n222/400, Train_loss: 0.3240\nTrain_dice: 0.6760\n223/400, Train_loss: 0.3172\nTrain_dice: 0.6828\n224/400, Train_loss: 0.3692\nTrain_dice: 0.6308\n225/400, Train_loss: 0.2405\nTrain_dice: 0.7595\n226/400, Train_loss: 0.3793\nTrain_dice: 0.6207\n227/400, Train_loss: 0.2882\nTrain_dice: 0.7118\n228/400, Train_loss: 0.2999\nTrain_dice: 0.7001\n229/400, Train_loss: 0.2247\nTrain_dice: 0.7753\n230/400, Train_loss: 0.2643\nTrain_dice: 0.7357\n231/400, Train_loss: 0.2325\nTrain_dice: 0.7675\n232/400, Train_loss: 0.2683\nTrain_dice: 0.7317\n233/400, Train_loss: 0.4576\nTrain_dice: 0.5424\n234/400, Train_loss: 0.2834\nTrain_dice: 0.7166\n235/400, Train_loss: 0.2387\nTrain_dice: 0.7613\n236/400, Train_loss: 0.2780\nTrain_dice: 0.7220\n237/400, Train_loss: 0.2327\nTrain_dice: 0.7673\n238/400, Train_loss: 0.4657\nTrain_dice: 0.5343\n239/400, Train_loss: 0.2628\nTrain_dice: 0.7372\n240/400, Train_loss: 0.3666\nTrain_dice: 0.6334\n241/400, Train_loss: 0.3120\nTrain_dice: 0.6880\n242/400, Train_loss: 0.3026\nTrain_dice: 0.6974\n243/400, Train_loss: 0.2426\nTrain_dice: 0.7574\n244/400, Train_loss: 0.2141\nTrain_dice: 0.7859\n245/400, Train_loss: 0.3270\nTrain_dice: 0.6730\n246/400, Train_loss: 0.3164\nTrain_dice: 0.6836\n247/400, Train_loss: 0.3214\nTrain_dice: 0.6786\n248/400, Train_loss: 0.3981\nTrain_dice: 0.6019\n249/400, Train_loss: 0.2239\nTrain_dice: 0.7761\n250/400, Train_loss: 0.2970\nTrain_dice: 0.7030\n251/400, Train_loss: 0.2772\nTrain_dice: 0.7228\n252/400, Train_loss: 0.2670\nTrain_dice: 0.7330\n253/400, Train_loss: 0.4206\nTrain_dice: 0.5794\n254/400, Train_loss: 0.3591\nTrain_dice: 0.6409\n255/400, Train_loss: 0.2189\nTrain_dice: 0.7811\n256/400, Train_loss: 0.2287\nTrain_dice: 0.7713\n257/400, Train_loss: 0.3447\nTrain_dice: 0.6553\n258/400, Train_loss: 0.3306\nTrain_dice: 0.6694\n259/400, Train_loss: 0.2259\nTrain_dice: 0.7741\n260/400, Train_loss: 0.2079\nTrain_dice: 0.7921\n261/400, Train_loss: 0.2095\nTrain_dice: 0.7905\n262/400, Train_loss: 0.2238\nTrain_dice: 0.7762\n263/400, Train_loss: 0.2302\nTrain_dice: 0.7698\n264/400, Train_loss: 0.2672\nTrain_dice: 0.7328\n265/400, Train_loss: 0.2123\nTrain_dice: 0.7877\n266/400, Train_loss: 0.2155\nTrain_dice: 0.7845\n267/400, Train_loss: 0.2349\nTrain_dice: 0.7651\n268/400, Train_loss: 0.3165\nTrain_dice: 0.6835\n269/400, Train_loss: 0.2534\nTrain_dice: 0.7466\n270/400, Train_loss: 0.2933\nTrain_dice: 0.7067\n271/400, Train_loss: 0.3372\nTrain_dice: 0.6628\n272/400, Train_loss: 0.2859\nTrain_dice: 0.7141\n273/400, Train_loss: 0.2976\nTrain_dice: 0.7024\n274/400, Train_loss: 0.2370\nTrain_dice: 0.7630\n275/400, Train_loss: 0.2476\nTrain_dice: 0.7524\n276/400, Train_loss: 0.2606\nTrain_dice: 0.7394\n277/400, Train_loss: 0.2357\nTrain_dice: 0.7643\n278/400, Train_loss: 0.3124\nTrain_dice: 0.6876\n279/400, Train_loss: 0.2416\nTrain_dice: 0.7584\n280/400, Train_loss: 0.2406\nTrain_dice: 0.7594\n281/400, Train_loss: 0.2535\nTrain_dice: 0.7465\n282/400, Train_loss: 0.2992\nTrain_dice: 0.7008\n283/400, Train_loss: 0.2313\nTrain_dice: 0.7687\n284/400, Train_loss: 0.2139\nTrain_dice: 0.7861\n285/400, Train_loss: 0.2480\nTrain_dice: 0.7520\n286/400, Train_loss: 0.2323\nTrain_dice: 0.7677\n287/400, Train_loss: 0.2412\nTrain_dice: 0.7588\n288/400, Train_loss: 0.2484\nTrain_dice: 0.7516\n289/400, Train_loss: 0.2514\nTrain_dice: 0.7486\n290/400, Train_loss: 0.3376\nTrain_dice: 0.6624\n291/400, Train_loss: 0.2836\nTrain_dice: 0.7164\n292/400, Train_loss: 0.3379\nTrain_dice: 0.6621\n293/400, Train_loss: 0.2282\nTrain_dice: 0.7718\n294/400, Train_loss: 0.2229\nTrain_dice: 0.7771\n295/400, Train_loss: 0.2669\nTrain_dice: 0.7331\n296/400, Train_loss: 0.2736\nTrain_dice: 0.7264\n297/400, Train_loss: 0.3469\nTrain_dice: 0.6531\n298/400, Train_loss: 0.2088\nTrain_dice: 0.7912\n299/400, Train_loss: 0.3487\nTrain_dice: 0.6513\n300/400, Train_loss: 0.2527\nTrain_dice: 0.7473\n301/400, Train_loss: 0.2814\nTrain_dice: 0.7186\n302/400, Train_loss: 0.3253\nTrain_dice: 0.6747\n303/400, Train_loss: 0.2963\nTrain_dice: 0.7037\n304/400, Train_loss: 0.3072\nTrain_dice: 0.6928\n305/400, Train_loss: 0.3064\nTrain_dice: 0.6936\n306/400, Train_loss: 0.3057\nTrain_dice: 0.6943\n307/400, Train_loss: 0.3396\nTrain_dice: 0.6604\n308/400, Train_loss: 0.2537\nTrain_dice: 0.7463\n309/400, Train_loss: 0.2325\nTrain_dice: 0.7675\n310/400, Train_loss: 0.2457\nTrain_dice: 0.7543\n311/400, Train_loss: 0.2492\nTrain_dice: 0.7508\n312/400, Train_loss: 0.3541\nTrain_dice: 0.6459\n313/400, Train_loss: 0.2249\nTrain_dice: 0.7751\n314/400, Train_loss: 0.2562\nTrain_dice: 0.7438\n315/400, Train_loss: 0.3584\nTrain_dice: 0.6416\n316/400, Train_loss: 0.2819\nTrain_dice: 0.7181\n317/400, Train_loss: 0.3842\nTrain_dice: 0.6158\n318/400, Train_loss: 0.2834\nTrain_dice: 0.7166\n319/400, Train_loss: 0.3129\nTrain_dice: 0.6871\n320/400, Train_loss: 0.2440\nTrain_dice: 0.7560\n321/400, Train_loss: 0.3231\nTrain_dice: 0.6769\n322/400, Train_loss: 0.3030\nTrain_dice: 0.6970\n323/400, Train_loss: 0.2688\nTrain_dice: 0.7312\n324/400, Train_loss: 0.3270\nTrain_dice: 0.6730\n325/400, Train_loss: 0.3213\nTrain_dice: 0.6787\n326/400, Train_loss: 0.2465\nTrain_dice: 0.7535\n327/400, Train_loss: 0.1893\nTrain_dice: 0.8107\n328/400, Train_loss: 0.2184\nTrain_dice: 0.7816\n329/400, Train_loss: 0.3606\nTrain_dice: 0.6394\n330/400, Train_loss: 0.2845\nTrain_dice: 0.7155\n331/400, Train_loss: 0.2406\nTrain_dice: 0.7594\n332/400, Train_loss: 0.2785\nTrain_dice: 0.7215\n333/400, Train_loss: 0.2467\nTrain_dice: 0.7533\n334/400, Train_loss: 0.2430\nTrain_dice: 0.7570\n335/400, Train_loss: 0.2608\nTrain_dice: 0.7392\n336/400, Train_loss: 0.2354\nTrain_dice: 0.7646\n337/400, Train_loss: 0.3565\nTrain_dice: 0.6435\n338/400, Train_loss: 0.4070\nTrain_dice: 0.5930\n339/400, Train_loss: 0.2665\nTrain_dice: 0.7335\n340/400, Train_loss: 0.3438\nTrain_dice: 0.6562\n341/400, Train_loss: 0.2809\nTrain_dice: 0.7191\n342/400, Train_loss: 0.3386\nTrain_dice: 0.6614\n343/400, Train_loss: 0.2376\nTrain_dice: 0.7624\n344/400, Train_loss: 0.2795\nTrain_dice: 0.7205\n345/400, Train_loss: 0.2492\nTrain_dice: 0.7508\n346/400, Train_loss: 0.2523\nTrain_dice: 0.7477\n347/400, Train_loss: 0.2154\nTrain_dice: 0.7846\n348/400, Train_loss: 0.3025\nTrain_dice: 0.6975\n349/400, Train_loss: 0.2187\nTrain_dice: 0.7813\n350/400, Train_loss: 0.3598\nTrain_dice: 0.6402\n351/400, Train_loss: 0.2168\nTrain_dice: 0.7832\n352/400, Train_loss: 0.3071\nTrain_dice: 0.6929\n353/400, Train_loss: 0.3231\nTrain_dice: 0.6769\n354/400, Train_loss: 0.2434\nTrain_dice: 0.7566\n355/400, Train_loss: 0.3359\nTrain_dice: 0.6641\n356/400, Train_loss: 0.2461\nTrain_dice: 0.7539\n357/400, Train_loss: 0.2706\nTrain_dice: 0.7294\n358/400, Train_loss: 0.2932\nTrain_dice: 0.7068\n359/400, Train_loss: 0.3450\nTrain_dice: 0.6550\n360/400, Train_loss: 0.3172\nTrain_dice: 0.6828\n361/400, Train_loss: 0.2395\nTrain_dice: 0.7605\n362/400, Train_loss: 0.2434\nTrain_dice: 0.7566\n363/400, Train_loss: 0.3549\nTrain_dice: 0.6451\n364/400, Train_loss: 0.3762\nTrain_dice: 0.6238\n365/400, Train_loss: 0.2494\nTrain_dice: 0.7506\n366/400, Train_loss: 0.3236\nTrain_dice: 0.6764\n367/400, Train_loss: 0.2486\nTrain_dice: 0.7514\n368/400, Train_loss: 0.3252\nTrain_dice: 0.6748\n369/400, Train_loss: 0.2219\nTrain_dice: 0.7781\n370/400, Train_loss: 0.2556\nTrain_dice: 0.7444\n371/400, Train_loss: 0.2556\nTrain_dice: 0.7444\n372/400, Train_loss: 0.3500\nTrain_dice: 0.6500\n373/400, Train_loss: 0.2506\nTrain_dice: 0.7494\n374/400, Train_loss: 0.2631\nTrain_dice: 0.7369\n375/400, Train_loss: 0.2397\nTrain_dice: 0.7603\n376/400, Train_loss: 0.3126\nTrain_dice: 0.6874\n377/400, Train_loss: 0.2301\nTrain_dice: 0.7699\n378/400, Train_loss: 0.2727\nTrain_dice: 0.7273\n379/400, Train_loss: 0.2376\nTrain_dice: 0.7624\n380/400, Train_loss: 0.3133\nTrain_dice: 0.6867\n381/400, Train_loss: 0.2821\nTrain_dice: 0.7179\n382/400, Train_loss: 0.3181\nTrain_dice: 0.6819\n383/400, Train_loss: 0.2778\nTrain_dice: 0.7222\n384/400, Train_loss: 0.2659\nTrain_dice: 0.7341\n385/400, Train_loss: 0.1984\nTrain_dice: 0.8016\n386/400, Train_loss: 0.3324\nTrain_dice: 0.6676\n387/400, Train_loss: 0.2867\nTrain_dice: 0.7133\n388/400, Train_loss: 0.1958\nTrain_dice: 0.8042\n389/400, Train_loss: 0.3187\nTrain_dice: 0.6813\n390/400, Train_loss: 0.2351\nTrain_dice: 0.7649\n391/400, Train_loss: 0.2104\nTrain_dice: 0.7896\n392/400, Train_loss: 0.2239\nTrain_dice: 0.7761\n393/400, Train_loss: 0.3363\nTrain_dice: 0.6637\n394/400, Train_loss: 0.2451\nTrain_dice: 0.7549\n395/400, Train_loss: 0.2188\nTrain_dice: 0.7812\n396/400, Train_loss: 0.2055\nTrain_dice: 0.7945\n397/400, Train_loss: 0.2141\nTrain_dice: 0.7859\n398/400, Train_loss: 0.2143\nTrain_dice: 0.7857\n399/400, Train_loss: 0.2272\nTrain_dice: 0.7728\n400/400, Train_loss: 0.2105\nTrain_dice: 0.7895\n--------------------\nEpoch_loss: 0.2919\nEpoch_metric: 0.7081\ntest_loss_epoch: 0.4399\ntest_dice_epoch: 0.5601\ncurrent epoch: 2 current mean dice: 0.5546\nbest mean dice: 0.5601 at epoch: 2\n----------\nepoch 3/600\n1/400, Train_loss: 0.2348\nTrain_dice: 0.7652\n2/400, Train_loss: 0.2155\nTrain_dice: 0.7845\n3/400, Train_loss: 0.2427\nTrain_dice: 0.7573\n4/400, Train_loss: 0.1983\nTrain_dice: 0.8017\n5/400, Train_loss: 0.2982\nTrain_dice: 0.7018\n6/400, Train_loss: 0.2173\nTrain_dice: 0.7827\n7/400, Train_loss: 0.3303\nTrain_dice: 0.6697\n8/400, Train_loss: 0.2319\nTrain_dice: 0.7681\n9/400, Train_loss: 0.2210\nTrain_dice: 0.7790\n10/400, Train_loss: 0.3004\nTrain_dice: 0.6996\n11/400, Train_loss: 0.2102\nTrain_dice: 0.7898\n12/400, Train_loss: 0.2301\nTrain_dice: 0.7699\n13/400, Train_loss: 0.2264\nTrain_dice: 0.7736\n14/400, Train_loss: 0.2200\nTrain_dice: 0.7800\n15/400, Train_loss: 0.2252\nTrain_dice: 0.7748\n16/400, Train_loss: 0.2321\nTrain_dice: 0.7679\n17/400, Train_loss: 0.2674\nTrain_dice: 0.7326\n18/400, Train_loss: 0.2315\nTrain_dice: 0.7685\n19/400, Train_loss: 0.3292\nTrain_dice: 0.6708\n20/400, Train_loss: 0.2267\nTrain_dice: 0.7733\n21/400, Train_loss: 0.2339\nTrain_dice: 0.7661\n22/400, Train_loss: 0.1905\nTrain_dice: 0.8095\n23/400, Train_loss: 0.2615\nTrain_dice: 0.7385\n24/400, Train_loss: 0.2238\nTrain_dice: 0.7762\n25/400, Train_loss: 0.2001\nTrain_dice: 0.7999\n26/400, Train_loss: 0.2116\nTrain_dice: 0.7884\n27/400, Train_loss: 0.2132\nTrain_dice: 0.7868\n28/400, Train_loss: 0.2249\nTrain_dice: 0.7751\n29/400, Train_loss: 0.2404\nTrain_dice: 0.7596\n30/400, Train_loss: 0.2001\nTrain_dice: 0.7999\n31/400, Train_loss: 0.3813\nTrain_dice: 0.6187\n32/400, Train_loss: 0.2504\nTrain_dice: 0.7496\n33/400, Train_loss: 0.2408\nTrain_dice: 0.7592\n34/400, Train_loss: 0.2249\nTrain_dice: 0.7751\n35/400, Train_loss: 0.2773\nTrain_dice: 0.7227\n36/400, Train_loss: 0.1909\nTrain_dice: 0.8091\n37/400, Train_loss: 0.3012\nTrain_dice: 0.6988\n38/400, Train_loss: 0.2407\nTrain_dice: 0.7593\n39/400, Train_loss: 0.2407\nTrain_dice: 0.7593\n40/400, Train_loss: 0.1938\nTrain_dice: 0.8062\n41/400, Train_loss: 0.2628\nTrain_dice: 0.7372\n42/400, Train_loss: 0.3791\nTrain_dice: 0.6209\n43/400, Train_loss: 0.2871\nTrain_dice: 0.7129\n44/400, Train_loss: 0.3278\nTrain_dice: 0.6722\n45/400, Train_loss: 0.1917\nTrain_dice: 0.8083\n46/400, Train_loss: 0.2146\nTrain_dice: 0.7854\n47/400, Train_loss: 0.2375\nTrain_dice: 0.7625\n48/400, Train_loss: 0.2315\nTrain_dice: 0.7685\n49/400, Train_loss: 0.3130\nTrain_dice: 0.6870\n50/400, Train_loss: 0.2140\nTrain_dice: 0.7860\n51/400, Train_loss: 0.1928\nTrain_dice: 0.8072\n52/400, Train_loss: 0.2202\nTrain_dice: 0.7798\n53/400, Train_loss: 0.3186\nTrain_dice: 0.6814\n54/400, Train_loss: 0.2096\nTrain_dice: 0.7904\n55/400, Train_loss: 0.2674\nTrain_dice: 0.7326\n56/400, Train_loss: 0.2300\nTrain_dice: 0.7700\n57/400, Train_loss: 0.2946\nTrain_dice: 0.7054\n58/400, Train_loss: 0.2961\nTrain_dice: 0.7039\n59/400, Train_loss: 0.2123\nTrain_dice: 0.7877\n60/400, Train_loss: 0.2223\nTrain_dice: 0.7777\n61/400, Train_loss: 0.2119\nTrain_dice: 0.7881\n62/400, Train_loss: 0.4373\nTrain_dice: 0.5627\n63/400, Train_loss: 0.1948\nTrain_dice: 0.8052\n64/400, Train_loss: 0.2106\nTrain_dice: 0.7894\n65/400, Train_loss: 0.2898\nTrain_dice: 0.7102\n66/400, Train_loss: 0.3099\nTrain_dice: 0.6901\n67/400, Train_loss: 0.2330\nTrain_dice: 0.7670\n68/400, Train_loss: 0.3457\nTrain_dice: 0.6543\n69/400, Train_loss: 0.3311\nTrain_dice: 0.6689\n70/400, Train_loss: 0.3216\nTrain_dice: 0.6784\n71/400, Train_loss: 0.3146\nTrain_dice: 0.6854\n72/400, Train_loss: 0.2934\nTrain_dice: 0.7066\n73/400, Train_loss: 0.2739\nTrain_dice: 0.7261\n74/400, Train_loss: 0.2620\nTrain_dice: 0.7380\n75/400, Train_loss: 0.2744\nTrain_dice: 0.7256\n76/400, Train_loss: 0.2110\nTrain_dice: 0.7890\n77/400, Train_loss: 0.4704\nTrain_dice: 0.5296\n78/400, Train_loss: 0.3795\nTrain_dice: 0.6205\n79/400, Train_loss: 0.2672\nTrain_dice: 0.7328\n80/400, Train_loss: 0.2918\nTrain_dice: 0.7082\n81/400, Train_loss: 0.2273\nTrain_dice: 0.7727\n82/400, Train_loss: 0.1917\nTrain_dice: 0.8083\n83/400, Train_loss: 0.2678\nTrain_dice: 0.7322\n84/400, Train_loss: 0.4717\nTrain_dice: 0.5283\n85/400, Train_loss: 0.1942\nTrain_dice: 0.8058\n86/400, Train_loss: 0.2462\nTrain_dice: 0.7538\n87/400, Train_loss: 0.3515\nTrain_dice: 0.6485\n88/400, Train_loss: 0.2112\nTrain_dice: 0.7888\n89/400, Train_loss: 0.1978\nTrain_dice: 0.8022\n90/400, Train_loss: 0.3164\nTrain_dice: 0.6836\n91/400, Train_loss: 0.1977\nTrain_dice: 0.8023\n92/400, Train_loss: 0.2762\nTrain_dice: 0.7238\n93/400, Train_loss: 0.2427\nTrain_dice: 0.7573\n94/400, Train_loss: 0.2025\nTrain_dice: 0.7975\n95/400, Train_loss: 0.4161\nTrain_dice: 0.5839\n96/400, Train_loss: 0.2372\nTrain_dice: 0.7628\n97/400, Train_loss: 0.1990\nTrain_dice: 0.8010\n98/400, Train_loss: 0.2025\nTrain_dice: 0.7975\n99/400, Train_loss: 0.2922\nTrain_dice: 0.7078\n100/400, Train_loss: 0.3177\nTrain_dice: 0.6823\n101/400, Train_loss: 0.4053\nTrain_dice: 0.5947\n102/400, Train_loss: 0.2505\nTrain_dice: 0.7495\n103/400, Train_loss: 0.3083\nTrain_dice: 0.6917\n104/400, Train_loss: 0.2996\nTrain_dice: 0.7004\n105/400, Train_loss: 0.2976\nTrain_dice: 0.7024\n106/400, Train_loss: 0.4030\nTrain_dice: 0.5970\n107/400, Train_loss: 0.3489\nTrain_dice: 0.6511\n108/400, Train_loss: 0.3476\nTrain_dice: 0.6524\n109/400, Train_loss: 0.1988\nTrain_dice: 0.8012\n110/400, Train_loss: 0.2153\nTrain_dice: 0.7847\n111/400, Train_loss: 0.1959\nTrain_dice: 0.8041\n112/400, Train_loss: 0.1898\nTrain_dice: 0.8102\n113/400, Train_loss: 0.2077\nTrain_dice: 0.7923\n114/400, Train_loss: 0.2394\nTrain_dice: 0.7606\n115/400, Train_loss: 0.2174\nTrain_dice: 0.7826\n116/400, Train_loss: 0.2064\nTrain_dice: 0.7936\n117/400, Train_loss: 0.2659\nTrain_dice: 0.7341\n118/400, Train_loss: 0.2409\nTrain_dice: 0.7591\n119/400, Train_loss: 0.3294\nTrain_dice: 0.6706\n120/400, Train_loss: 0.2656\nTrain_dice: 0.7344\n121/400, Train_loss: 0.2516\nTrain_dice: 0.7484\n122/400, Train_loss: 0.2580\nTrain_dice: 0.7420\n123/400, Train_loss: 0.3910\nTrain_dice: 0.6090\n124/400, Train_loss: 0.3035\nTrain_dice: 0.6965\n125/400, Train_loss: 0.3480\nTrain_dice: 0.6520\n126/400, Train_loss: 0.2583\nTrain_dice: 0.7417\n127/400, Train_loss: 0.1994\nTrain_dice: 0.8006\n128/400, Train_loss: 0.2204\nTrain_dice: 0.7796\n129/400, Train_loss: 0.2167\nTrain_dice: 0.7833\n130/400, Train_loss: 0.2628\nTrain_dice: 0.7372\n131/400, Train_loss: 0.2641\nTrain_dice: 0.7359\n132/400, Train_loss: 0.2362\nTrain_dice: 0.7638\n133/400, Train_loss: 0.2479\nTrain_dice: 0.7521\n134/400, Train_loss: 0.3247\nTrain_dice: 0.6753\n135/400, Train_loss: 0.2631\nTrain_dice: 0.7369\n136/400, Train_loss: 0.3366\nTrain_dice: 0.6634\n137/400, Train_loss: 0.3487\nTrain_dice: 0.6513\n138/400, Train_loss: 0.4753\nTrain_dice: 0.5247\n139/400, Train_loss: 0.3339\nTrain_dice: 0.6661\n140/400, Train_loss: 0.2913\nTrain_dice: 0.7087\n141/400, Train_loss: 0.2652\nTrain_dice: 0.7348\n142/400, Train_loss: 0.2088\nTrain_dice: 0.7912\n143/400, Train_loss: 0.1954\nTrain_dice: 0.8046\n144/400, Train_loss: 0.3176\nTrain_dice: 0.6824\n145/400, Train_loss: 0.2020\nTrain_dice: 0.7980\n146/400, Train_loss: 0.2263\nTrain_dice: 0.7737\n147/400, Train_loss: 0.3129\nTrain_dice: 0.6871\n148/400, Train_loss: 0.2666\nTrain_dice: 0.7334\n149/400, Train_loss: 0.3021\nTrain_dice: 0.6979\n150/400, Train_loss: 0.2908\nTrain_dice: 0.7092\n151/400, Train_loss: 0.3060\nTrain_dice: 0.6940\n152/400, Train_loss: 0.2993\nTrain_dice: 0.7007\n153/400, Train_loss: 0.2927\nTrain_dice: 0.7073\n154/400, Train_loss: 0.3901\nTrain_dice: 0.6099\n155/400, Train_loss: 0.2325\nTrain_dice: 0.7675\n156/400, Train_loss: 0.3286\nTrain_dice: 0.6714\n157/400, Train_loss: 0.3651\nTrain_dice: 0.6349\n158/400, Train_loss: 0.2240\nTrain_dice: 0.7760\n159/400, Train_loss: 0.2337\nTrain_dice: 0.7663\n160/400, Train_loss: 0.4049\nTrain_dice: 0.5951\n161/400, Train_loss: 0.2768\nTrain_dice: 0.7232\n162/400, Train_loss: 0.2226\nTrain_dice: 0.7774\n163/400, Train_loss: 0.3190\nTrain_dice: 0.6810\n164/400, Train_loss: 0.2404\nTrain_dice: 0.7596\n165/400, Train_loss: 0.2477\nTrain_dice: 0.7523\n166/400, Train_loss: 0.1901\nTrain_dice: 0.8099\n167/400, Train_loss: 0.2031\nTrain_dice: 0.7969\n168/400, Train_loss: 0.3015\nTrain_dice: 0.6985\n169/400, Train_loss: 0.2484\nTrain_dice: 0.7516\n170/400, Train_loss: 0.2498\nTrain_dice: 0.7502\n171/400, Train_loss: 0.2302\nTrain_dice: 0.7698\n172/400, Train_loss: 0.1920\nTrain_dice: 0.8080\n173/400, Train_loss: 0.1894\nTrain_dice: 0.8106\n174/400, Train_loss: 0.2183\nTrain_dice: 0.7817\n175/400, Train_loss: 0.3493\nTrain_dice: 0.6507\n176/400, Train_loss: 0.3349\nTrain_dice: 0.6651\n177/400, Train_loss: 0.3650\nTrain_dice: 0.6350\n178/400, Train_loss: 0.5056\nTrain_dice: 0.4944\n179/400, Train_loss: 0.4319\nTrain_dice: 0.5681\n180/400, Train_loss: 0.3236\nTrain_dice: 0.6764\n181/400, Train_loss: 0.2069\nTrain_dice: 0.7931\n182/400, Train_loss: 0.1926\nTrain_dice: 0.8074\n183/400, Train_loss: 0.3085\nTrain_dice: 0.6915\n184/400, Train_loss: 0.3499\nTrain_dice: 0.6501\n185/400, Train_loss: 0.3127\nTrain_dice: 0.6873\n186/400, Train_loss: 0.3724\nTrain_dice: 0.6276\n187/400, Train_loss: 0.3359\nTrain_dice: 0.6641\n188/400, Train_loss: 0.3131\nTrain_dice: 0.6869\n189/400, Train_loss: 0.3446\nTrain_dice: 0.6554\n190/400, Train_loss: 0.2176\nTrain_dice: 0.7824\n191/400, Train_loss: 0.2095\nTrain_dice: 0.7905\n192/400, Train_loss: 0.3529\nTrain_dice: 0.6471\n193/400, Train_loss: 0.2194\nTrain_dice: 0.7806\n194/400, Train_loss: 0.3179\nTrain_dice: 0.6821\n195/400, Train_loss: 0.2463\nTrain_dice: 0.7537\n196/400, Train_loss: 0.2300\nTrain_dice: 0.7700\n197/400, Train_loss: 0.3724\nTrain_dice: 0.6276\n198/400, Train_loss: 0.2016\nTrain_dice: 0.7984\n199/400, Train_loss: 0.2216\nTrain_dice: 0.7784\n200/400, Train_loss: 0.2101\nTrain_dice: 0.7899\n201/400, Train_loss: 0.2585\nTrain_dice: 0.7415\n202/400, Train_loss: 0.2113\nTrain_dice: 0.7887\n203/400, Train_loss: 0.2954\nTrain_dice: 0.7046\n204/400, Train_loss: 0.2747\nTrain_dice: 0.7253\n205/400, Train_loss: 0.2393\nTrain_dice: 0.7607\n206/400, Train_loss: 0.2309\nTrain_dice: 0.7691\n207/400, Train_loss: 0.2695\nTrain_dice: 0.7305\n208/400, Train_loss: 0.2272\nTrain_dice: 0.7728\n209/400, Train_loss: 0.2680\nTrain_dice: 0.7320\n210/400, Train_loss: 0.2537\nTrain_dice: 0.7463\n211/400, Train_loss: 0.3413\nTrain_dice: 0.6587\n212/400, Train_loss: 0.4025\nTrain_dice: 0.5975\n213/400, Train_loss: 0.3818\nTrain_dice: 0.6182\n214/400, Train_loss: 0.3374\nTrain_dice: 0.6626\n215/400, Train_loss: 0.3578\nTrain_dice: 0.6422\n216/400, Train_loss: 0.3724\nTrain_dice: 0.6276\n217/400, Train_loss: 0.2075\nTrain_dice: 0.7925\n218/400, Train_loss: 0.2001\nTrain_dice: 0.7999\n219/400, Train_loss: 0.2695\nTrain_dice: 0.7305\n220/400, Train_loss: 0.2515\nTrain_dice: 0.7485\n221/400, Train_loss: 0.2328\nTrain_dice: 0.7672\n222/400, Train_loss: 0.2996\nTrain_dice: 0.7004\n223/400, Train_loss: 0.2951\nTrain_dice: 0.7049\n224/400, Train_loss: 0.3504\nTrain_dice: 0.6496\n225/400, Train_loss: 0.2122\nTrain_dice: 0.7878\n226/400, Train_loss: 0.3631\nTrain_dice: 0.6369\n227/400, Train_loss: 0.2591\nTrain_dice: 0.7409\n228/400, Train_loss: 0.2741\nTrain_dice: 0.7259\n229/400, Train_loss: 0.1919\nTrain_dice: 0.8081\n230/400, Train_loss: 0.2356\nTrain_dice: 0.7644\n231/400, Train_loss: 0.2034\nTrain_dice: 0.7966\n232/400, Train_loss: 0.2429\nTrain_dice: 0.7571\n233/400, Train_loss: 0.4422\nTrain_dice: 0.5578\n234/400, Train_loss: 0.2465\nTrain_dice: 0.7535\n235/400, Train_loss: 0.2132\nTrain_dice: 0.7868\n236/400, Train_loss: 0.2522\nTrain_dice: 0.7478\n237/400, Train_loss: 0.1972\nTrain_dice: 0.8028\n238/400, Train_loss: 0.4531\nTrain_dice: 0.5469\n239/400, Train_loss: 0.2348\nTrain_dice: 0.7652\n240/400, Train_loss: 0.3478\nTrain_dice: 0.6522\n241/400, Train_loss: 0.2822\nTrain_dice: 0.7178\n242/400, Train_loss: 0.2744\nTrain_dice: 0.7256\n243/400, Train_loss: 0.2102\nTrain_dice: 0.7898\n244/400, Train_loss: 0.1777\nTrain_dice: 0.8223\n245/400, Train_loss: 0.2961\nTrain_dice: 0.7039\n246/400, Train_loss: 0.2862\nTrain_dice: 0.7138\n247/400, Train_loss: 0.2899\nTrain_dice: 0.7101\n248/400, Train_loss: 0.3802\nTrain_dice: 0.6198\n249/400, Train_loss: 0.1923\nTrain_dice: 0.8077\n250/400, Train_loss: 0.2681\nTrain_dice: 0.7319\n251/400, Train_loss: 0.2485\nTrain_dice: 0.7515\n252/400, Train_loss: 0.2386\nTrain_dice: 0.7614\n253/400, Train_loss: 0.4025\nTrain_dice: 0.5975\n254/400, Train_loss: 0.3311\nTrain_dice: 0.6689\n255/400, Train_loss: 0.1877\nTrain_dice: 0.8123\n256/400, Train_loss: 0.1921\nTrain_dice: 0.8079\n257/400, Train_loss: 0.3165\nTrain_dice: 0.6835\n258/400, Train_loss: 0.3030\nTrain_dice: 0.6970\n259/400, Train_loss: 0.1922\nTrain_dice: 0.8078\n260/400, Train_loss: 0.1785\nTrain_dice: 0.8215\n261/400, Train_loss: 0.1783\nTrain_dice: 0.8217\n262/400, Train_loss: 0.1949\nTrain_dice: 0.8051\n263/400, Train_loss: 0.1957\nTrain_dice: 0.8043\n264/400, Train_loss: 0.2403\nTrain_dice: 0.7597\n265/400, Train_loss: 0.1798\nTrain_dice: 0.8202\n266/400, Train_loss: 0.1854\nTrain_dice: 0.8146\n267/400, Train_loss: 0.2002\nTrain_dice: 0.7998\n268/400, Train_loss: 0.2770\nTrain_dice: 0.7230\n269/400, Train_loss: 0.2222\nTrain_dice: 0.7778\n270/400, Train_loss: 0.2625\nTrain_dice: 0.7375\n271/400, Train_loss: 0.3079\nTrain_dice: 0.6921\n272/400, Train_loss: 0.2632\nTrain_dice: 0.7368\n273/400, Train_loss: 0.2716\nTrain_dice: 0.7284\n274/400, Train_loss: 0.2142\nTrain_dice: 0.7858\n275/400, Train_loss: 0.2170\nTrain_dice: 0.7830\n276/400, Train_loss: 0.2315\nTrain_dice: 0.7685\n277/400, Train_loss: 0.2068\nTrain_dice: 0.7932\n278/400, Train_loss: 0.2947\nTrain_dice: 0.7053\n279/400, Train_loss: 0.2101\nTrain_dice: 0.7899\n280/400, Train_loss: 0.2112\nTrain_dice: 0.7888\n281/400, Train_loss: 0.2196\nTrain_dice: 0.7804\n282/400, Train_loss: 0.2748\nTrain_dice: 0.7252\n283/400, Train_loss: 0.1965\nTrain_dice: 0.8035\n284/400, Train_loss: 0.1789\nTrain_dice: 0.8211\n285/400, Train_loss: 0.2247\nTrain_dice: 0.7753\n286/400, Train_loss: 0.2023\nTrain_dice: 0.7977\n287/400, Train_loss: 0.2150\nTrain_dice: 0.7850\n288/400, Train_loss: 0.2191\nTrain_dice: 0.7809\n289/400, Train_loss: 0.2220\nTrain_dice: 0.7780\n290/400, Train_loss: 0.3123\nTrain_dice: 0.6877\n291/400, Train_loss: 0.2601\nTrain_dice: 0.7399\n292/400, Train_loss: 0.3217\nTrain_dice: 0.6783\n293/400, Train_loss: 0.1968\nTrain_dice: 0.8032\n294/400, Train_loss: 0.1874\nTrain_dice: 0.8126\n295/400, Train_loss: 0.2346\nTrain_dice: 0.7654\n296/400, Train_loss: 0.2364\nTrain_dice: 0.7636\n297/400, Train_loss: 0.3211\nTrain_dice: 0.6789\n298/400, Train_loss: 0.1713\nTrain_dice: 0.8287\n299/400, Train_loss: 0.3331\nTrain_dice: 0.6669\n300/400, Train_loss: 0.2213\nTrain_dice: 0.7787\n301/400, Train_loss: 0.2452\nTrain_dice: 0.7548\n302/400, Train_loss: 0.2999\nTrain_dice: 0.7001\n303/400, Train_loss: 0.2621\nTrain_dice: 0.7379\n304/400, Train_loss: 0.2769\nTrain_dice: 0.7231\n305/400, Train_loss: 0.2715\nTrain_dice: 0.7285\n306/400, Train_loss: 0.2780\nTrain_dice: 0.7220\n307/400, Train_loss: 0.3068\nTrain_dice: 0.6932\n308/400, Train_loss: 0.2175\nTrain_dice: 0.7825\n309/400, Train_loss: 0.1950\nTrain_dice: 0.8050\n310/400, Train_loss: 0.2107\nTrain_dice: 0.7893\n311/400, Train_loss: 0.2087\nTrain_dice: 0.7913\n312/400, Train_loss: 0.3255\nTrain_dice: 0.6745\n313/400, Train_loss: 0.1953\nTrain_dice: 0.8047\n314/400, Train_loss: 0.2205\nTrain_dice: 0.7795\n315/400, Train_loss: 0.3397\nTrain_dice: 0.6603\n316/400, Train_loss: 0.2475\nTrain_dice: 0.7525\n317/400, Train_loss: 0.3695\nTrain_dice: 0.6305\n318/400, Train_loss: 0.2521\nTrain_dice: 0.7479\n319/400, Train_loss: 0.2820\nTrain_dice: 0.7180\n320/400, Train_loss: 0.2205\nTrain_dice: 0.7795\n321/400, Train_loss: 0.2920\nTrain_dice: 0.7080\n322/400, Train_loss: 0.2700\nTrain_dice: 0.7300\n323/400, Train_loss: 0.2272\nTrain_dice: 0.7728\n324/400, Train_loss: 0.3015\nTrain_dice: 0.6985\n325/400, Train_loss: 0.2860\nTrain_dice: 0.7140\n326/400, Train_loss: 0.2140\nTrain_dice: 0.7860\n327/400, Train_loss: 0.1587\nTrain_dice: 0.8413\n328/400, Train_loss: 0.1883\nTrain_dice: 0.8117\n329/400, Train_loss: 0.3457\nTrain_dice: 0.6543\n330/400, Train_loss: 0.2568\nTrain_dice: 0.7432\n331/400, Train_loss: 0.2097\nTrain_dice: 0.7903\n332/400, Train_loss: 0.2549\nTrain_dice: 0.7451\n333/400, Train_loss: 0.2145\nTrain_dice: 0.7855\n334/400, Train_loss: 0.2124\nTrain_dice: 0.7876\n335/400, Train_loss: 0.2281\nTrain_dice: 0.7719\n336/400, Train_loss: 0.2058\nTrain_dice: 0.7942\n337/400, Train_loss: 0.3349\nTrain_dice: 0.6651\n338/400, Train_loss: 0.3915\nTrain_dice: 0.6085\n339/400, Train_loss: 0.2399\nTrain_dice: 0.7601\n340/400, Train_loss: 0.3269\nTrain_dice: 0.6731\n341/400, Train_loss: 0.2546\nTrain_dice: 0.7454\n342/400, Train_loss: 0.3194\nTrain_dice: 0.6806\n343/400, Train_loss: 0.2084\nTrain_dice: 0.7916\n344/400, Train_loss: 0.2505\nTrain_dice: 0.7495\n345/400, Train_loss: 0.2155\nTrain_dice: 0.7845\n346/400, Train_loss: 0.2246\nTrain_dice: 0.7754\n347/400, Train_loss: 0.1770\nTrain_dice: 0.8230\n348/400, Train_loss: 0.2798\nTrain_dice: 0.7202\n349/400, Train_loss: 0.1828\nTrain_dice: 0.8172\n350/400, Train_loss: 0.3376\nTrain_dice: 0.6624\n351/400, Train_loss: 0.1834\nTrain_dice: 0.8166\n352/400, Train_loss: 0.2856\nTrain_dice: 0.7144\n353/400, Train_loss: 0.2934\nTrain_dice: 0.7066\n354/400, Train_loss: 0.2105\nTrain_dice: 0.7895\n355/400, Train_loss: 0.3098\nTrain_dice: 0.6902\n356/400, Train_loss: 0.2075\nTrain_dice: 0.7925\n357/400, Train_loss: 0.2368\nTrain_dice: 0.7632\n358/400, Train_loss: 0.2551\nTrain_dice: 0.7449\n359/400, Train_loss: 0.3189\nTrain_dice: 0.6811\n360/400, Train_loss: 0.2833\nTrain_dice: 0.7167\n361/400, Train_loss: 0.2028\nTrain_dice: 0.7972\n362/400, Train_loss: 0.2083\nTrain_dice: 0.7917\n363/400, Train_loss: 0.3291\nTrain_dice: 0.6709\n364/400, Train_loss: 0.3526\nTrain_dice: 0.6474\n365/400, Train_loss: 0.2101\nTrain_dice: 0.7899\n366/400, Train_loss: 0.3044\nTrain_dice: 0.6956\n367/400, Train_loss: 0.2229\nTrain_dice: 0.7771\n368/400, Train_loss: 0.2992\nTrain_dice: 0.7008\n369/400, Train_loss: 0.1926\nTrain_dice: 0.8074\n370/400, Train_loss: 0.2189\nTrain_dice: 0.7811\n371/400, Train_loss: 0.2204\nTrain_dice: 0.7796\n372/400, Train_loss: 0.3274\nTrain_dice: 0.6726\n373/400, Train_loss: 0.2279\nTrain_dice: 0.7721\n374/400, Train_loss: 0.2303\nTrain_dice: 0.7697\n375/400, Train_loss: 0.2023\nTrain_dice: 0.7977\n376/400, Train_loss: 0.2840\nTrain_dice: 0.7160\n377/400, Train_loss: 0.1898\nTrain_dice: 0.8102\n378/400, Train_loss: 0.2427\nTrain_dice: 0.7573\n379/400, Train_loss: 0.2089\nTrain_dice: 0.7911\n380/400, Train_loss: 0.2930\nTrain_dice: 0.7070\n381/400, Train_loss: 0.2615\nTrain_dice: 0.7385\n382/400, Train_loss: 0.2927\nTrain_dice: 0.7073\n383/400, Train_loss: 0.2573\nTrain_dice: 0.7427\n384/400, Train_loss: 0.2453\nTrain_dice: 0.7547\n385/400, Train_loss: 0.1727\nTrain_dice: 0.8273\n386/400, Train_loss: 0.3127\nTrain_dice: 0.6873\n387/400, Train_loss: 0.2649\nTrain_dice: 0.7351\n388/400, Train_loss: 0.1664\nTrain_dice: 0.8336\n389/400, Train_loss: 0.2928\nTrain_dice: 0.7072\n390/400, Train_loss: 0.2090\nTrain_dice: 0.7910\n391/400, Train_loss: 0.1766\nTrain_dice: 0.8234\n392/400, Train_loss: 0.1934\nTrain_dice: 0.8066\n393/400, Train_loss: 0.3076\nTrain_dice: 0.6924\n394/400, Train_loss: 0.2120\nTrain_dice: 0.7880\n395/400, Train_loss: 0.1941\nTrain_dice: 0.8059\n396/400, Train_loss: 0.1756\nTrain_dice: 0.8244\n397/400, Train_loss: 0.1886\nTrain_dice: 0.8114\n398/400, Train_loss: 0.1869\nTrain_dice: 0.8131\n399/400, Train_loss: 0.2007\nTrain_dice: 0.7993\n400/400, Train_loss: 0.1773\nTrain_dice: 0.8227\n--------------------\nEpoch_loss: 0.2617\nEpoch_metric: 0.7383\ntest_loss_epoch: 0.4254\ntest_dice_epoch: 0.5746\ncurrent epoch: 3 current mean dice: 0.5684\nbest mean dice: 0.5746 at epoch: 3\n----------\nepoch 4/600\n1/400, Train_loss: 0.2050\nTrain_dice: 0.7950\n2/400, Train_loss: 0.1861\nTrain_dice: 0.8139\n3/400, Train_loss: 0.2105\nTrain_dice: 0.7895\n4/400, Train_loss: 0.1664\nTrain_dice: 0.8336\n5/400, Train_loss: 0.2765\nTrain_dice: 0.7235\n6/400, Train_loss: 0.1860\nTrain_dice: 0.8140\n7/400, Train_loss: 0.3070\nTrain_dice: 0.6930\n8/400, Train_loss: 0.2032\nTrain_dice: 0.7968\n9/400, Train_loss: 0.1955\nTrain_dice: 0.8045\n10/400, Train_loss: 0.2783\nTrain_dice: 0.7217\n11/400, Train_loss: 0.1782\nTrain_dice: 0.8218\n12/400, Train_loss: 0.2019\nTrain_dice: 0.7981\n13/400, Train_loss: 0.1958\nTrain_dice: 0.8042\n14/400, Train_loss: 0.1956\nTrain_dice: 0.8044\n15/400, Train_loss: 0.1961\nTrain_dice: 0.8039\n16/400, Train_loss: 0.2053\nTrain_dice: 0.7947\n17/400, Train_loss: 0.2476\nTrain_dice: 0.7524\n18/400, Train_loss: 0.2081\nTrain_dice: 0.7919\n19/400, Train_loss: 0.3120\nTrain_dice: 0.6880\n20/400, Train_loss: 0.2009\nTrain_dice: 0.7991\n21/400, Train_loss: 0.2110\nTrain_dice: 0.7890\n22/400, Train_loss: 0.1566\nTrain_dice: 0.8434\n23/400, Train_loss: 0.2342\nTrain_dice: 0.7658\n24/400, Train_loss: 0.1992\nTrain_dice: 0.8008\n25/400, Train_loss: 0.1740\nTrain_dice: 0.8260\n26/400, Train_loss: 0.1806\nTrain_dice: 0.8194\n27/400, Train_loss: 0.1838\nTrain_dice: 0.8162\n28/400, Train_loss: 0.2032\nTrain_dice: 0.7968\n29/400, Train_loss: 0.2183\nTrain_dice: 0.7817\n30/400, Train_loss: 0.1773\nTrain_dice: 0.8227\n31/400, Train_loss: 0.3622\nTrain_dice: 0.6378\n32/400, Train_loss: 0.2177\nTrain_dice: 0.7823\n33/400, Train_loss: 0.2126\nTrain_dice: 0.7874\n34/400, Train_loss: 0.1884\nTrain_dice: 0.8116\n35/400, Train_loss: 0.2529\nTrain_dice: 0.7471\n36/400, Train_loss: 0.1670\nTrain_dice: 0.8330\n37/400, Train_loss: 0.2780\nTrain_dice: 0.7220\n38/400, Train_loss: 0.2140\nTrain_dice: 0.7860\n39/400, Train_loss: 0.2237\nTrain_dice: 0.7763\n40/400, Train_loss: 0.1660\nTrain_dice: 0.8340\n41/400, Train_loss: 0.2280\nTrain_dice: 0.7720\n42/400, Train_loss: 0.3550\nTrain_dice: 0.6450\n43/400, Train_loss: 0.2593\nTrain_dice: 0.7407\n44/400, Train_loss: 0.2989\nTrain_dice: 0.7011\n45/400, Train_loss: 0.1614\nTrain_dice: 0.8386\n46/400, Train_loss: 0.1831\nTrain_dice: 0.8169\n47/400, Train_loss: 0.2139\nTrain_dice: 0.7861\n48/400, Train_loss: 0.1961\nTrain_dice: 0.8039\n49/400, Train_loss: 0.2869\nTrain_dice: 0.7131\n50/400, Train_loss: 0.1890\nTrain_dice: 0.8110\n51/400, Train_loss: 0.1666\nTrain_dice: 0.8334\n52/400, Train_loss: 0.1850\nTrain_dice: 0.8150\n53/400, Train_loss: 0.2962\nTrain_dice: 0.7038\n54/400, Train_loss: 0.1819\nTrain_dice: 0.8181\n55/400, Train_loss: 0.2400\nTrain_dice: 0.7600\n56/400, Train_loss: 0.2024\nTrain_dice: 0.7976\n57/400, Train_loss: 0.2664\nTrain_dice: 0.7336\n58/400, Train_loss: 0.2654\nTrain_dice: 0.7346\n59/400, Train_loss: 0.1881\nTrain_dice: 0.8119\n60/400, Train_loss: 0.1920\nTrain_dice: 0.8080\n61/400, Train_loss: 0.1817\nTrain_dice: 0.8183\n62/400, Train_loss: 0.4273\nTrain_dice: 0.5727\n63/400, Train_loss: 0.1671\nTrain_dice: 0.8329\n64/400, Train_loss: 0.1812\nTrain_dice: 0.8188\n65/400, Train_loss: 0.2640\nTrain_dice: 0.7360\n66/400, Train_loss: 0.2837\nTrain_dice: 0.7163\n67/400, Train_loss: 0.1990\nTrain_dice: 0.8010\n68/400, Train_loss: 0.3209\nTrain_dice: 0.6791\n69/400, Train_loss: 0.3062\nTrain_dice: 0.6938\n70/400, Train_loss: 0.2933\nTrain_dice: 0.7067\n71/400, Train_loss: 0.2928\nTrain_dice: 0.7072\n72/400, Train_loss: 0.2692\nTrain_dice: 0.7308\n73/400, Train_loss: 0.2508\nTrain_dice: 0.7492\n74/400, Train_loss: 0.2337\nTrain_dice: 0.7663\n75/400, Train_loss: 0.2453\nTrain_dice: 0.7547\n76/400, Train_loss: 0.1768\nTrain_dice: 0.8232\n77/400, Train_loss: 0.4605\nTrain_dice: 0.5395\n78/400, Train_loss: 0.3623\nTrain_dice: 0.6377\n79/400, Train_loss: 0.2440\nTrain_dice: 0.7560\n80/400, Train_loss: 0.2677\nTrain_dice: 0.7323\n81/400, Train_loss: 0.1971\nTrain_dice: 0.8029\n82/400, Train_loss: 0.1700\nTrain_dice: 0.8300\n83/400, Train_loss: 0.2449\nTrain_dice: 0.7551\n84/400, Train_loss: 0.4617\nTrain_dice: 0.5383\n85/400, Train_loss: 0.1667\nTrain_dice: 0.8333\n86/400, Train_loss: 0.2164\nTrain_dice: 0.7836\n87/400, Train_loss: 0.3299\nTrain_dice: 0.6701\n88/400, Train_loss: 0.1858\nTrain_dice: 0.8142\n89/400, Train_loss: 0.1685\nTrain_dice: 0.8315\n90/400, Train_loss: 0.2974\nTrain_dice: 0.7026\n91/400, Train_loss: 0.1670\nTrain_dice: 0.8330\n92/400, Train_loss: 0.2469\nTrain_dice: 0.7531\n93/400, Train_loss: 0.2110\nTrain_dice: 0.7890\n94/400, Train_loss: 0.1786\nTrain_dice: 0.8214\n95/400, Train_loss: 0.4004\nTrain_dice: 0.5996\n96/400, Train_loss: 0.2195\nTrain_dice: 0.7805\n97/400, Train_loss: 0.1682\nTrain_dice: 0.8318\n98/400, Train_loss: 0.1712\nTrain_dice: 0.8288\n99/400, Train_loss: 0.2670\nTrain_dice: 0.7330\n100/400, Train_loss: 0.2985\nTrain_dice: 0.7015\n101/400, Train_loss: 0.3965\nTrain_dice: 0.6035\n102/400, Train_loss: 0.2244\nTrain_dice: 0.7756\n103/400, Train_loss: 0.2862\nTrain_dice: 0.7138\n104/400, Train_loss: 0.2729\nTrain_dice: 0.7271\n105/400, Train_loss: 0.2712\nTrain_dice: 0.7288\n106/400, Train_loss: 0.3881\nTrain_dice: 0.6119\n107/400, Train_loss: 0.3258\nTrain_dice: 0.6742\n108/400, Train_loss: 0.3230\nTrain_dice: 0.6770\n109/400, Train_loss: 0.1687\nTrain_dice: 0.8313\n110/400, Train_loss: 0.1821\nTrain_dice: 0.8179\n111/400, Train_loss: 0.1652\nTrain_dice: 0.8348\n112/400, Train_loss: 0.1543\nTrain_dice: 0.8457\n113/400, Train_loss: 0.1785\nTrain_dice: 0.8215\n114/400, Train_loss: 0.2108\nTrain_dice: 0.7892\n115/400, Train_loss: 0.1831\nTrain_dice: 0.8169\n116/400, Train_loss: 0.1717\nTrain_dice: 0.8283\n117/400, Train_loss: 0.2328\nTrain_dice: 0.7672\n118/400, Train_loss: 0.2140\nTrain_dice: 0.7860\n119/400, Train_loss: 0.3103\nTrain_dice: 0.6897\n120/400, Train_loss: 0.2425\nTrain_dice: 0.7575\n121/400, Train_loss: 0.2250\nTrain_dice: 0.7750\n122/400, Train_loss: 0.2259\nTrain_dice: 0.7741\n123/400, Train_loss: 0.3696\nTrain_dice: 0.6304\n124/400, Train_loss: 0.2694\nTrain_dice: 0.7306\n125/400, Train_loss: 0.3207\nTrain_dice: 0.6793\n126/400, Train_loss: 0.2341\nTrain_dice: 0.7659\n127/400, Train_loss: 0.1690\nTrain_dice: 0.8310\n128/400, Train_loss: 0.1921\nTrain_dice: 0.8079\n129/400, Train_loss: 0.1843\nTrain_dice: 0.8157\n130/400, Train_loss: 0.2371\nTrain_dice: 0.7629\n131/400, Train_loss: 0.2409\nTrain_dice: 0.7591\n132/400, Train_loss: 0.2123\nTrain_dice: 0.7877\n133/400, Train_loss: 0.2217\nTrain_dice: 0.7783\n134/400, Train_loss: 0.3018\nTrain_dice: 0.6982\n135/400, Train_loss: 0.2344\nTrain_dice: 0.7656\n136/400, Train_loss: 0.3200\nTrain_dice: 0.6800\n137/400, Train_loss: 0.3293\nTrain_dice: 0.6707\n138/400, Train_loss: 0.4684\nTrain_dice: 0.5316\n139/400, Train_loss: 0.3138\nTrain_dice: 0.6862\n140/400, Train_loss: 0.2725\nTrain_dice: 0.7275\n141/400, Train_loss: 0.2426\nTrain_dice: 0.7574\n142/400, Train_loss: 0.1761\nTrain_dice: 0.8239\n143/400, Train_loss: 0.1645\nTrain_dice: 0.8355\n144/400, Train_loss: 0.2873\nTrain_dice: 0.7127\n145/400, Train_loss: 0.1711\nTrain_dice: 0.8289\n146/400, Train_loss: 0.1971\nTrain_dice: 0.8029\n147/400, Train_loss: 0.2886\nTrain_dice: 0.7114\n148/400, Train_loss: 0.2357\nTrain_dice: 0.7643\n149/400, Train_loss: 0.2723\nTrain_dice: 0.7277\n150/400, Train_loss: 0.2603\nTrain_dice: 0.7397\n151/400, Train_loss: 0.2787\nTrain_dice: 0.7213\n152/400, Train_loss: 0.2714\nTrain_dice: 0.7286\n153/400, Train_loss: 0.2654\nTrain_dice: 0.7346\n154/400, Train_loss: 0.3735\nTrain_dice: 0.6265\n155/400, Train_loss: 0.2041\nTrain_dice: 0.7959\n156/400, Train_loss: 0.3062\nTrain_dice: 0.6938\n157/400, Train_loss: 0.3477\nTrain_dice: 0.6523\n158/400, Train_loss: 0.1914\nTrain_dice: 0.8086\n159/400, Train_loss: 0.1993\nTrain_dice: 0.8007\n160/400, Train_loss: 0.3885\nTrain_dice: 0.6115\n161/400, Train_loss: 0.2498\nTrain_dice: 0.7502\n162/400, Train_loss: 0.1912\nTrain_dice: 0.8088\n163/400, Train_loss: 0.2912\nTrain_dice: 0.7088\n164/400, Train_loss: 0.2107\nTrain_dice: 0.7893\n165/400, Train_loss: 0.2148\nTrain_dice: 0.7852\n166/400, Train_loss: 0.1694\nTrain_dice: 0.8306\n167/400, Train_loss: 0.1721\nTrain_dice: 0.8279\n168/400, Train_loss: 0.2763\nTrain_dice: 0.7237\n169/400, Train_loss: 0.2122\nTrain_dice: 0.7878\n170/400, Train_loss: 0.2197\nTrain_dice: 0.7803\n171/400, Train_loss: 0.2017\nTrain_dice: 0.7983\n172/400, Train_loss: 0.1608\nTrain_dice: 0.8392\n173/400, Train_loss: 0.1613\nTrain_dice: 0.8387\n174/400, Train_loss: 0.1845\nTrain_dice: 0.8155\n175/400, Train_loss: 0.3257\nTrain_dice: 0.6743\n176/400, Train_loss: 0.3102\nTrain_dice: 0.6898\n177/400, Train_loss: 0.3443\nTrain_dice: 0.6557\n178/400, Train_loss: 0.4983\nTrain_dice: 0.5017\n179/400, Train_loss: 0.4179\nTrain_dice: 0.5821\n180/400, Train_loss: 0.3013\nTrain_dice: 0.6987\n181/400, Train_loss: 0.1763\nTrain_dice: 0.8237\n182/400, Train_loss: 0.1646\nTrain_dice: 0.8354\n183/400, Train_loss: 0.2988\nTrain_dice: 0.7012\n184/400, Train_loss: 0.3369\nTrain_dice: 0.6631\n185/400, Train_loss: 0.2962\nTrain_dice: 0.7038\n186/400, Train_loss: 0.3612\nTrain_dice: 0.6388\n187/400, Train_loss: 0.3210\nTrain_dice: 0.6790\n188/400, Train_loss: 0.2980\nTrain_dice: 0.7020\n189/400, Train_loss: 0.3268\nTrain_dice: 0.6732\n190/400, Train_loss: 0.1847\nTrain_dice: 0.8153\n191/400, Train_loss: 0.1778\nTrain_dice: 0.8222\n192/400, Train_loss: 0.3351\nTrain_dice: 0.6649\n193/400, Train_loss: 0.1887\nTrain_dice: 0.8113\n194/400, Train_loss: 0.2911\nTrain_dice: 0.7089\n195/400, Train_loss: 0.2178\nTrain_dice: 0.7822\n196/400, Train_loss: 0.1956\nTrain_dice: 0.8044\n197/400, Train_loss: 0.3527\nTrain_dice: 0.6473\n198/400, Train_loss: 0.1656\nTrain_dice: 0.8344\n199/400, Train_loss: 0.1986\nTrain_dice: 0.8014\n200/400, Train_loss: 0.1776\nTrain_dice: 0.8224\n201/400, Train_loss: 0.2333\nTrain_dice: 0.7667\n202/400, Train_loss: 0.1788\nTrain_dice: 0.8212\n203/400, Train_loss: 0.2683\nTrain_dice: 0.7317\n204/400, Train_loss: 0.2518\nTrain_dice: 0.7482\n205/400, Train_loss: 0.2130\nTrain_dice: 0.7870\n206/400, Train_loss: 0.2029\nTrain_dice: 0.7971\n207/400, Train_loss: 0.2476\nTrain_dice: 0.7524\n208/400, Train_loss: 0.2004\nTrain_dice: 0.7996\n209/400, Train_loss: 0.2480\nTrain_dice: 0.7520\n210/400, Train_loss: 0.2270\nTrain_dice: 0.7730\n211/400, Train_loss: 0.3225\nTrain_dice: 0.6775\n212/400, Train_loss: 0.3871\nTrain_dice: 0.6129\n213/400, Train_loss: 0.3667\nTrain_dice: 0.6333\n214/400, Train_loss: 0.3156\nTrain_dice: 0.6844\n215/400, Train_loss: 0.3386\nTrain_dice: 0.6614\n216/400, Train_loss: 0.3531\nTrain_dice: 0.6469\n217/400, Train_loss: 0.1726\nTrain_dice: 0.8274\n218/400, Train_loss: 0.1760\nTrain_dice: 0.8240\n219/400, Train_loss: 0.2396\nTrain_dice: 0.7604\n220/400, Train_loss: 0.2212\nTrain_dice: 0.7788\n221/400, Train_loss: 0.2118\nTrain_dice: 0.7882\n222/400, Train_loss: 0.2740\nTrain_dice: 0.7260\n223/400, Train_loss: 0.2727\nTrain_dice: 0.7273\n224/400, Train_loss: 0.3305\nTrain_dice: 0.6695\n225/400, Train_loss: 0.1859\nTrain_dice: 0.8141\n226/400, Train_loss: 0.3466\nTrain_dice: 0.6534\n227/400, Train_loss: 0.2313\nTrain_dice: 0.7687\n228/400, Train_loss: 0.2490\nTrain_dice: 0.7510\n229/400, Train_loss: 0.1716\nTrain_dice: 0.8284\n230/400, Train_loss: 0.2088\nTrain_dice: 0.7912\n231/400, Train_loss: 0.1750\nTrain_dice: 0.8250\n232/400, Train_loss: 0.2183\nTrain_dice: 0.7817\n233/400, Train_loss: 0.4268\nTrain_dice: 0.5732\n234/400, Train_loss: 0.2087\nTrain_dice: 0.7913\n235/400, Train_loss: 0.1922\nTrain_dice: 0.8078\n236/400, Train_loss: 0.2275\nTrain_dice: 0.7725\n237/400, Train_loss: 0.1660\nTrain_dice: 0.8340\n238/400, Train_loss: 0.4391\nTrain_dice: 0.5609\n239/400, Train_loss: 0.2059\nTrain_dice: 0.7941\n240/400, Train_loss: 0.3294\nTrain_dice: 0.6706\n241/400, Train_loss: 0.2533\nTrain_dice: 0.7467\n242/400, Train_loss: 0.2452\nTrain_dice: 0.7548\n243/400, Train_loss: 0.1808\nTrain_dice: 0.8192\n244/400, Train_loss: 0.1477\nTrain_dice: 0.8523\n245/400, Train_loss: 0.2674\nTrain_dice: 0.7326\n246/400, Train_loss: 0.2591\nTrain_dice: 0.7409\n247/400, Train_loss: 0.2605\nTrain_dice: 0.7395\n248/400, Train_loss: 0.3623\nTrain_dice: 0.6377\n249/400, Train_loss: 0.1664\nTrain_dice: 0.8336\n250/400, Train_loss: 0.2419\nTrain_dice: 0.7581\n251/400, Train_loss: 0.2204\nTrain_dice: 0.7796\n252/400, Train_loss: 0.2077\nTrain_dice: 0.7923\n253/400, Train_loss: 0.3830\nTrain_dice: 0.6170\n254/400, Train_loss: 0.3012\nTrain_dice: 0.6988\n255/400, Train_loss: 0.1667\nTrain_dice: 0.8333\n256/400, Train_loss: 0.1631\nTrain_dice: 0.8369\n257/400, Train_loss: 0.2890\nTrain_dice: 0.7110\n258/400, Train_loss: 0.2730\nTrain_dice: 0.7270\n259/400, Train_loss: 0.1658\nTrain_dice: 0.8342\n260/400, Train_loss: 0.1589\nTrain_dice: 0.8411\n261/400, Train_loss: 0.1507\nTrain_dice: 0.8493\n262/400, Train_loss: 0.1710\nTrain_dice: 0.8290\n263/400, Train_loss: 0.1692\nTrain_dice: 0.8308\n264/400, Train_loss: 0.2154\nTrain_dice: 0.7846\n265/400, Train_loss: 0.1547\nTrain_dice: 0.8453\n266/400, Train_loss: 0.1640\nTrain_dice: 0.8360\n267/400, Train_loss: 0.1695\nTrain_dice: 0.8305\n268/400, Train_loss: 0.2417\nTrain_dice: 0.7583\n269/400, Train_loss: 0.1910\nTrain_dice: 0.8090\n270/400, Train_loss: 0.2331\nTrain_dice: 0.7669\n271/400, Train_loss: 0.2769\nTrain_dice: 0.7231\n272/400, Train_loss: 0.2375\nTrain_dice: 0.7625\n273/400, Train_loss: 0.2456\nTrain_dice: 0.7544\n274/400, Train_loss: 0.1945\nTrain_dice: 0.8055\n275/400, Train_loss: 0.1908\nTrain_dice: 0.8092\n276/400, Train_loss: 0.2069\nTrain_dice: 0.7931\n277/400, Train_loss: 0.1814\nTrain_dice: 0.8186\n278/400, Train_loss: 0.2740\nTrain_dice: 0.7260\n279/400, Train_loss: 0.1796\nTrain_dice: 0.8204\n280/400, Train_loss: 0.1862\nTrain_dice: 0.8138\n281/400, Train_loss: 0.1866\nTrain_dice: 0.8134\n282/400, Train_loss: 0.2539\nTrain_dice: 0.7461\n283/400, Train_loss: 0.1684\nTrain_dice: 0.8316\n284/400, Train_loss: 0.1499\nTrain_dice: 0.8501\n285/400, Train_loss: 0.2064\nTrain_dice: 0.7936\n286/400, Train_loss: 0.1766\nTrain_dice: 0.8234\n287/400, Train_loss: 0.1894\nTrain_dice: 0.8106\n288/400, Train_loss: 0.1886\nTrain_dice: 0.8114\n289/400, Train_loss: 0.1948\nTrain_dice: 0.8052\n290/400, Train_loss: 0.2831\nTrain_dice: 0.7169\n291/400, Train_loss: 0.2389\nTrain_dice: 0.7611\n292/400, Train_loss: 0.3091\nTrain_dice: 0.6909\n293/400, Train_loss: 0.1678\nTrain_dice: 0.8322\n294/400, Train_loss: 0.1552\nTrain_dice: 0.8448\n295/400, Train_loss: 0.2203\nTrain_dice: 0.7797\n296/400, Train_loss: 0.2040\nTrain_dice: 0.7960\n297/400, Train_loss: 0.2943\nTrain_dice: 0.7057\n298/400, Train_loss: 0.1472\nTrain_dice: 0.8528\n299/400, Train_loss: 0.3312\nTrain_dice: 0.6688\n300/400, Train_loss: 0.2052\nTrain_dice: 0.7948\n301/400, Train_loss: 0.2180\nTrain_dice: 0.7820\n302/400, Train_loss: 0.2785\nTrain_dice: 0.7215\n303/400, Train_loss: 0.2381\nTrain_dice: 0.7619\n304/400, Train_loss: 0.2542\nTrain_dice: 0.7458\n305/400, Train_loss: 0.2399\nTrain_dice: 0.7601\n306/400, Train_loss: 0.2532\nTrain_dice: 0.7468\n307/400, Train_loss: 0.2930\nTrain_dice: 0.7070\n308/400, Train_loss: 0.1903\nTrain_dice: 0.8097\n309/400, Train_loss: 0.1684\nTrain_dice: 0.8316\n310/400, Train_loss: 0.1845\nTrain_dice: 0.8155\n311/400, Train_loss: 0.1788\nTrain_dice: 0.8212\n312/400, Train_loss: 0.3079\nTrain_dice: 0.6921\n313/400, Train_loss: 0.1745\nTrain_dice: 0.8255\n314/400, Train_loss: 0.1984\nTrain_dice: 0.8016\n315/400, Train_loss: 0.3333\nTrain_dice: 0.6667\n316/400, Train_loss: 0.2201\nTrain_dice: 0.7799\n317/400, Train_loss: 0.3538\nTrain_dice: 0.6462\n318/400, Train_loss: 0.2241\nTrain_dice: 0.7759\n319/400, Train_loss: 0.2554\nTrain_dice: 0.7446\n320/400, Train_loss: 0.2105\nTrain_dice: 0.7895\n321/400, Train_loss: 0.2656\nTrain_dice: 0.7344\n322/400, Train_loss: 0.2489\nTrain_dice: 0.7511\n323/400, Train_loss: 0.1997\nTrain_dice: 0.8003\n324/400, Train_loss: 0.2876\nTrain_dice: 0.7124\n325/400, Train_loss: 0.2707\nTrain_dice: 0.7293\n326/400, Train_loss: 0.1911\nTrain_dice: 0.8089\n327/400, Train_loss: 0.1378\nTrain_dice: 0.8622\n328/400, Train_loss: 0.1672\nTrain_dice: 0.8328\n329/400, Train_loss: 0.3299\nTrain_dice: 0.6701\n330/400, Train_loss: 0.2332\nTrain_dice: 0.7668\n331/400, Train_loss: 0.1838\nTrain_dice: 0.8162\n332/400, Train_loss: 0.2336\nTrain_dice: 0.7664\n333/400, Train_loss: 0.1872\nTrain_dice: 0.8128\n334/400, Train_loss: 0.1874\nTrain_dice: 0.8126\n335/400, Train_loss: 0.1991\nTrain_dice: 0.8009\n336/400, Train_loss: 0.1820\nTrain_dice: 0.8180\n337/400, Train_loss: 0.3159\nTrain_dice: 0.6841\n338/400, Train_loss: 0.3783\nTrain_dice: 0.6217\n339/400, Train_loss: 0.2154\nTrain_dice: 0.7846\n340/400, Train_loss: 0.3112\nTrain_dice: 0.6888\n341/400, Train_loss: 0.2341\nTrain_dice: 0.7659\n342/400, Train_loss: 0.3027\nTrain_dice: 0.6973\n343/400, Train_loss: 0.1834\nTrain_dice: 0.8166\n344/400, Train_loss: 0.2270\nTrain_dice: 0.7730\n345/400, Train_loss: 0.1871\nTrain_dice: 0.8129\n346/400, Train_loss: 0.2000\nTrain_dice: 0.8000\n347/400, Train_loss: 0.1480\nTrain_dice: 0.8520\n348/400, Train_loss: 0.2600\nTrain_dice: 0.7400\n349/400, Train_loss: 0.1549\nTrain_dice: 0.8451\n350/400, Train_loss: 0.3181\nTrain_dice: 0.6819\n351/400, Train_loss: 0.1593\nTrain_dice: 0.8407\n352/400, Train_loss: 0.2692\nTrain_dice: 0.7308\n353/400, Train_loss: 0.2762\nTrain_dice: 0.7238\n354/400, Train_loss: 0.1831\nTrain_dice: 0.8169\n355/400, Train_loss: 0.2897\nTrain_dice: 0.7103\n356/400, Train_loss: 0.1782\nTrain_dice: 0.8218\n357/400, Train_loss: 0.2130\nTrain_dice: 0.7870\n358/400, Train_loss: 0.2287\nTrain_dice: 0.7713\n359/400, Train_loss: 0.2996\nTrain_dice: 0.7004\n360/400, Train_loss: 0.2553\nTrain_dice: 0.7447\n361/400, Train_loss: 0.1769\nTrain_dice: 0.8231\n362/400, Train_loss: 0.1817\nTrain_dice: 0.8183\n363/400, Train_loss: 0.3074\nTrain_dice: 0.6926\n364/400, Train_loss: 0.3348\nTrain_dice: 0.6652\n365/400, Train_loss: 0.1819\nTrain_dice: 0.8181\n366/400, Train_loss: 0.2901\nTrain_dice: 0.7099\n367/400, Train_loss: 0.2084\nTrain_dice: 0.7916\n368/400, Train_loss: 0.2792\nTrain_dice: 0.7208\n369/400, Train_loss: 0.1754\nTrain_dice: 0.8246\n370/400, Train_loss: 0.1952\nTrain_dice: 0.8048\n371/400, Train_loss: 0.1939\nTrain_dice: 0.8061\n372/400, Train_loss: 0.3065\nTrain_dice: 0.6935\n373/400, Train_loss: 0.2195\nTrain_dice: 0.7805\n374/400, Train_loss: 0.2112\nTrain_dice: 0.7888\n375/400, Train_loss: 0.1809\nTrain_dice: 0.8191\n376/400, Train_loss: 0.2621\nTrain_dice: 0.7379\n377/400, Train_loss: 0.1623\nTrain_dice: 0.8377\n378/400, Train_loss: 0.2152\nTrain_dice: 0.7848\n379/400, Train_loss: 0.1852\nTrain_dice: 0.8148\n380/400, Train_loss: 0.2753\nTrain_dice: 0.7247\n381/400, Train_loss: 0.2433\nTrain_dice: 0.7567\n382/400, Train_loss: 0.2723\nTrain_dice: 0.7277\n383/400, Train_loss: 0.2391\nTrain_dice: 0.7609\n384/400, Train_loss: 0.2279\nTrain_dice: 0.7721\n385/400, Train_loss: 0.1517\nTrain_dice: 0.8483\n386/400, Train_loss: 0.2935\nTrain_dice: 0.7065\n387/400, Train_loss: 0.2416\nTrain_dice: 0.7584\n388/400, Train_loss: 0.1424\nTrain_dice: 0.8576\n389/400, Train_loss: 0.2651\nTrain_dice: 0.7349\n390/400, Train_loss: 0.1870\nTrain_dice: 0.8130\n391/400, Train_loss: 0.1481\nTrain_dice: 0.8519\n392/400, Train_loss: 0.1667\nTrain_dice: 0.8333\n393/400, Train_loss: 0.2780\nTrain_dice: 0.7220\n394/400, Train_loss: 0.1895\nTrain_dice: 0.8105\n395/400, Train_loss: 0.1743\nTrain_dice: 0.8257\n396/400, Train_loss: 0.1535\nTrain_dice: 0.8465\n397/400, Train_loss: 0.1692\nTrain_dice: 0.8308\n398/400, Train_loss: 0.1635\nTrain_dice: 0.8365\n399/400, Train_loss: 0.1769\nTrain_dice: 0.8231\n400/400, Train_loss: 0.1516\nTrain_dice: 0.8484\n--------------------\nEpoch_loss: 0.2369\nEpoch_metric: 0.7631\ntest_loss_epoch: 0.4111\ntest_dice_epoch: 0.5889\ncurrent epoch: 4 current mean dice: 0.5845\nbest mean dice: 0.5889 at epoch: 4\n----------\nepoch 5/600\n1/400, Train_loss: 0.1808\nTrain_dice: 0.8192\n2/400, Train_loss: 0.1622\nTrain_dice: 0.8378\n3/400, Train_loss: 0.1834\nTrain_dice: 0.8166\n4/400, Train_loss: 0.1422\nTrain_dice: 0.8578\n5/400, Train_loss: 0.2628\nTrain_dice: 0.7372\n6/400, Train_loss: 0.1628\nTrain_dice: 0.8372\n7/400, Train_loss: 0.2858\nTrain_dice: 0.7142\n8/400, Train_loss: 0.1801\nTrain_dice: 0.8199\n9/400, Train_loss: 0.1742\nTrain_dice: 0.8258\n10/400, Train_loss: 0.2593\nTrain_dice: 0.7407\n11/400, Train_loss: 0.1524\nTrain_dice: 0.8476\n12/400, Train_loss: 0.1804\nTrain_dice: 0.8196\n13/400, Train_loss: 0.1705\nTrain_dice: 0.8295\n14/400, Train_loss: 0.1763\nTrain_dice: 0.8237\n15/400, Train_loss: 0.1724\nTrain_dice: 0.8276\n16/400, Train_loss: 0.1846\nTrain_dice: 0.8154\n17/400, Train_loss: 0.2320\nTrain_dice: 0.7680\n18/400, Train_loss: 0.1904\nTrain_dice: 0.8096\n19/400, Train_loss: 0.2986\nTrain_dice: 0.7014\n20/400, Train_loss: 0.1804\nTrain_dice: 0.8196\n21/400, Train_loss: 0.1928\nTrain_dice: 0.8072\n22/400, Train_loss: 0.1313\nTrain_dice: 0.8687\n23/400, Train_loss: 0.2094\nTrain_dice: 0.7906\n24/400, Train_loss: 0.1800\nTrain_dice: 0.8200\n25/400, Train_loss: 0.1538\nTrain_dice: 0.8462\n26/400, Train_loss: 0.1565\nTrain_dice: 0.8435\n27/400, Train_loss: 0.1603\nTrain_dice: 0.8397\n28/400, Train_loss: 0.1856\nTrain_dice: 0.8144\n29/400, Train_loss: 0.2009\nTrain_dice: 0.7991\n30/400, Train_loss: 0.1587\nTrain_dice: 0.8413\n31/400, Train_loss: 0.3486\nTrain_dice: 0.6514\n32/400, Train_loss: 0.1892\nTrain_dice: 0.8108\n33/400, Train_loss: 0.1861\nTrain_dice: 0.8139\n34/400, Train_loss: 0.1626\nTrain_dice: 0.8374\n35/400, Train_loss: 0.2305\nTrain_dice: 0.7695\n36/400, Train_loss: 0.1479\nTrain_dice: 0.8521\n37/400, Train_loss: 0.2581\nTrain_dice: 0.7419\n38/400, Train_loss: 0.1882\nTrain_dice: 0.8118\n39/400, Train_loss: 0.2090\nTrain_dice: 0.7910\n40/400, Train_loss: 0.1446\nTrain_dice: 0.8554\n41/400, Train_loss: 0.2013\nTrain_dice: 0.7987\n42/400, Train_loss: 0.3349\nTrain_dice: 0.6651\n43/400, Train_loss: 0.2332\nTrain_dice: 0.7668\n44/400, Train_loss: 0.2719\nTrain_dice: 0.7281\n45/400, Train_loss: 0.1394\nTrain_dice: 0.8606\n46/400, Train_loss: 0.1576\nTrain_dice: 0.8424\n47/400, Train_loss: 0.1953\nTrain_dice: 0.8047\n48/400, Train_loss: 0.1677\nTrain_dice: 0.8323\n49/400, Train_loss: 0.2677\nTrain_dice: 0.7323\n50/400, Train_loss: 0.1701\nTrain_dice: 0.8299\n51/400, Train_loss: 0.1497\nTrain_dice: 0.8503\n52/400, Train_loss: 0.1592\nTrain_dice: 0.8408\n53/400, Train_loss: 0.2760\nTrain_dice: 0.7240\n54/400, Train_loss: 0.1577\nTrain_dice: 0.8423\n55/400, Train_loss: 0.2131\nTrain_dice: 0.7869\n56/400, Train_loss: 0.1769\nTrain_dice: 0.8231\n57/400, Train_loss: 0.2382\nTrain_dice: 0.7618\n58/400, Train_loss: 0.2360\nTrain_dice: 0.7640\n59/400, Train_loss: 0.1686\nTrain_dice: 0.8314\n60/400, Train_loss: 0.1709\nTrain_dice: 0.8291\n61/400, Train_loss: 0.1605\nTrain_dice: 0.8395\n62/400, Train_loss: 0.4197\nTrain_dice: 0.5803\n63/400, Train_loss: 0.1455\nTrain_dice: 0.8545\n64/400, Train_loss: 0.1561\nTrain_dice: 0.8439\n65/400, Train_loss: 0.2452\nTrain_dice: 0.7548\n66/400, Train_loss: 0.2613\nTrain_dice: 0.7387\n67/400, Train_loss: 0.1728\nTrain_dice: 0.8272\n68/400, Train_loss: 0.2979\nTrain_dice: 0.7021\n69/400, Train_loss: 0.2813\nTrain_dice: 0.7187\n70/400, Train_loss: 0.2647\nTrain_dice: 0.7353\n71/400, Train_loss: 0.2715\nTrain_dice: 0.7285\n72/400, Train_loss: 0.2478\nTrain_dice: 0.7522\n73/400, Train_loss: 0.2276\nTrain_dice: 0.7724\n74/400, Train_loss: 0.2056\nTrain_dice: 0.7944\n75/400, Train_loss: 0.2212\nTrain_dice: 0.7788\n76/400, Train_loss: 0.1521\nTrain_dice: 0.8479\n77/400, Train_loss: 0.4533\nTrain_dice: 0.5467\n78/400, Train_loss: 0.3454\nTrain_dice: 0.6546\n79/400, Train_loss: 0.2231\nTrain_dice: 0.7769\n80/400, Train_loss: 0.2475\nTrain_dice: 0.7525\n81/400, Train_loss: 0.1711\nTrain_dice: 0.8289\n82/400, Train_loss: 0.1570\nTrain_dice: 0.8430\n83/400, Train_loss: 0.2238\nTrain_dice: 0.7762\n84/400, Train_loss: 0.4538\nTrain_dice: 0.5462\n85/400, Train_loss: 0.1450\nTrain_dice: 0.8550\n86/400, Train_loss: 0.1934\nTrain_dice: 0.8066\n87/400, Train_loss: 0.3099\nTrain_dice: 0.6901\n88/400, Train_loss: 0.1655\nTrain_dice: 0.8345\n89/400, Train_loss: 0.1471\nTrain_dice: 0.8529\n90/400, Train_loss: 0.2794\nTrain_dice: 0.7206\n91/400, Train_loss: 0.1452\nTrain_dice: 0.8548\n92/400, Train_loss: 0.2236\nTrain_dice: 0.7764\n93/400, Train_loss: 0.1854\nTrain_dice: 0.8146\n94/400, Train_loss: 0.1625\nTrain_dice: 0.8375\n95/400, Train_loss: 0.3857\nTrain_dice: 0.6143\n96/400, Train_loss: 0.2067\nTrain_dice: 0.7933\n97/400, Train_loss: 0.1436\nTrain_dice: 0.8564\n98/400, Train_loss: 0.1494\nTrain_dice: 0.8506\n99/400, Train_loss: 0.2472\nTrain_dice: 0.7528\n100/400, Train_loss: 0.2820\nTrain_dice: 0.7180\n101/400, Train_loss: 0.3903\nTrain_dice: 0.6097\n102/400, Train_loss: 0.2019\nTrain_dice: 0.7981\n103/400, Train_loss: 0.2677\nTrain_dice: 0.7323\n104/400, Train_loss: 0.2522\nTrain_dice: 0.7478\n105/400, Train_loss: 0.2463\nTrain_dice: 0.7537\n106/400, Train_loss: 0.3751\nTrain_dice: 0.6249\n107/400, Train_loss: 0.3018\nTrain_dice: 0.6982\n108/400, Train_loss: 0.2984\nTrain_dice: 0.7016\n109/400, Train_loss: 0.1491\nTrain_dice: 0.8509\n110/400, Train_loss: 0.1584\nTrain_dice: 0.8416\n111/400, Train_loss: 0.1429\nTrain_dice: 0.8571\n112/400, Train_loss: 0.1307\nTrain_dice: 0.8693\n113/400, Train_loss: 0.1559\nTrain_dice: 0.8441\n114/400, Train_loss: 0.1887\nTrain_dice: 0.8113\n115/400, Train_loss: 0.1590\nTrain_dice: 0.8410\n116/400, Train_loss: 0.1444\nTrain_dice: 0.8556\n117/400, Train_loss: 0.2045\nTrain_dice: 0.7955\n118/400, Train_loss: 0.1914\nTrain_dice: 0.8086\n119/400, Train_loss: 0.2910\nTrain_dice: 0.7090\n120/400, Train_loss: 0.2250\nTrain_dice: 0.7750\n121/400, Train_loss: 0.2059\nTrain_dice: 0.7941\n122/400, Train_loss: 0.1995\nTrain_dice: 0.8005\n123/400, Train_loss: 0.3517\nTrain_dice: 0.6483\n124/400, Train_loss: 0.2408\nTrain_dice: 0.7592\n125/400, Train_loss: 0.2985\nTrain_dice: 0.7015\n126/400, Train_loss: 0.2152\nTrain_dice: 0.7848\n127/400, Train_loss: 0.1491\nTrain_dice: 0.8509\n128/400, Train_loss: 0.1698\nTrain_dice: 0.8302\n129/400, Train_loss: 0.1628\nTrain_dice: 0.8372\n130/400, Train_loss: 0.2141\nTrain_dice: 0.7859\n131/400, Train_loss: 0.2164\nTrain_dice: 0.7836\n132/400, Train_loss: 0.1927\nTrain_dice: 0.8073\n133/400, Train_loss: 0.2011\nTrain_dice: 0.7989\n134/400, Train_loss: 0.2811\nTrain_dice: 0.7189\n135/400, Train_loss: 0.2118\nTrain_dice: 0.7882\n136/400, Train_loss: 0.3048\nTrain_dice: 0.6952\n137/400, Train_loss: 0.3136\nTrain_dice: 0.6864\n138/400, Train_loss: 0.4622\nTrain_dice: 0.5378\n139/400, Train_loss: 0.2972\nTrain_dice: 0.7028\n140/400, Train_loss: 0.2561\nTrain_dice: 0.7439\n141/400, Train_loss: 0.2240\nTrain_dice: 0.7760\n142/400, Train_loss: 0.1546\nTrain_dice: 0.8454\n143/400, Train_loss: 0.1457\nTrain_dice: 0.8543\n144/400, Train_loss: 0.2592\nTrain_dice: 0.7408\n145/400, Train_loss: 0.1497\nTrain_dice: 0.8503\n146/400, Train_loss: 0.1745\nTrain_dice: 0.8255\n147/400, Train_loss: 0.2682\nTrain_dice: 0.7318\n148/400, Train_loss: 0.2092\nTrain_dice: 0.7908\n149/400, Train_loss: 0.2450\nTrain_dice: 0.7550\n150/400, Train_loss: 0.2345\nTrain_dice: 0.7655\n151/400, Train_loss: 0.2522\nTrain_dice: 0.7478\n152/400, Train_loss: 0.2448\nTrain_dice: 0.7552\n153/400, Train_loss: 0.2391\nTrain_dice: 0.7609\n154/400, Train_loss: 0.3591\nTrain_dice: 0.6409\n155/400, Train_loss: 0.1866\nTrain_dice: 0.8134\n156/400, Train_loss: 0.2867\nTrain_dice: 0.7133\n157/400, Train_loss: 0.3317\nTrain_dice: 0.6683\n158/400, Train_loss: 0.1677\nTrain_dice: 0.8323\n159/400, Train_loss: 0.1728\nTrain_dice: 0.8272\n160/400, Train_loss: 0.3739\nTrain_dice: 0.6261\n161/400, Train_loss: 0.2305\nTrain_dice: 0.7695\n162/400, Train_loss: 0.1675\nTrain_dice: 0.8325\n163/400, Train_loss: 0.2692\nTrain_dice: 0.7308\n164/400, Train_loss: 0.1855\nTrain_dice: 0.8145\n165/400, Train_loss: 0.1899\nTrain_dice: 0.8101\n166/400, Train_loss: 0.1579\nTrain_dice: 0.8421\n167/400, Train_loss: 0.1522\nTrain_dice: 0.8478\n168/400, Train_loss: 0.2555\nTrain_dice: 0.7445\n169/400, Train_loss: 0.1867\nTrain_dice: 0.8133\n170/400, Train_loss: 0.1953\nTrain_dice: 0.8047\n171/400, Train_loss: 0.1819\nTrain_dice: 0.8181\n172/400, Train_loss: 0.1397\nTrain_dice: 0.8603\n173/400, Train_loss: 0.1448\nTrain_dice: 0.8552\n174/400, Train_loss: 0.1634\nTrain_dice: 0.8366\n175/400, Train_loss: 0.3046\nTrain_dice: 0.6954\n176/400, Train_loss: 0.2890\nTrain_dice: 0.7110\n177/400, Train_loss: 0.3247\nTrain_dice: 0.6753\n178/400, Train_loss: 0.4915\nTrain_dice: 0.5085\n179/400, Train_loss: 0.4044\nTrain_dice: 0.5956\n180/400, Train_loss: 0.2821\nTrain_dice: 0.7179\n181/400, Train_loss: 0.1562\nTrain_dice: 0.8438\n182/400, Train_loss: 0.1463\nTrain_dice: 0.8537\n183/400, Train_loss: 0.2876\nTrain_dice: 0.7124\n184/400, Train_loss: 0.3255\nTrain_dice: 0.6745\n185/400, Train_loss: 0.2820\nTrain_dice: 0.7180\n186/400, Train_loss: 0.3500\nTrain_dice: 0.6500\n187/400, Train_loss: 0.3084\nTrain_dice: 0.6916\n188/400, Train_loss: 0.2837\nTrain_dice: 0.7163\n189/400, Train_loss: 0.3100\nTrain_dice: 0.6900\n190/400, Train_loss: 0.1639\nTrain_dice: 0.8361\n191/400, Train_loss: 0.1581\nTrain_dice: 0.8419\n192/400, Train_loss: 0.3180\nTrain_dice: 0.6820\n193/400, Train_loss: 0.1667\nTrain_dice: 0.8333\n194/400, Train_loss: 0.2697\nTrain_dice: 0.7303\n195/400, Train_loss: 0.1959\nTrain_dice: 0.8041\n196/400, Train_loss: 0.1713\nTrain_dice: 0.8287\n197/400, Train_loss: 0.3393\nTrain_dice: 0.6607\n198/400, Train_loss: 0.1434\nTrain_dice: 0.8566\n199/400, Train_loss: 0.1813\nTrain_dice: 0.8187\n200/400, Train_loss: 0.1554\nTrain_dice: 0.8446\n201/400, Train_loss: 0.2160\nTrain_dice: 0.7840\n202/400, Train_loss: 0.1554\nTrain_dice: 0.8446\n203/400, Train_loss: 0.2418\nTrain_dice: 0.7582\n204/400, Train_loss: 0.2310\nTrain_dice: 0.7690\n205/400, Train_loss: 0.1924\nTrain_dice: 0.8076\n206/400, Train_loss: 0.1808\nTrain_dice: 0.8192\n207/400, Train_loss: 0.2311\nTrain_dice: 0.7689\n208/400, Train_loss: 0.1798\nTrain_dice: 0.8202\n209/400, Train_loss: 0.2317\nTrain_dice: 0.7683\n210/400, Train_loss: 0.2024\nTrain_dice: 0.7976\n211/400, Train_loss: 0.3043\nTrain_dice: 0.6957\n212/400, Train_loss: 0.3717\nTrain_dice: 0.6283\n213/400, Train_loss: 0.3499\nTrain_dice: 0.6501\n214/400, Train_loss: 0.2948\nTrain_dice: 0.7052\n215/400, Train_loss: 0.3212\nTrain_dice: 0.6788\n216/400, Train_loss: 0.3348\nTrain_dice: 0.6652\n217/400, Train_loss: 0.1521\nTrain_dice: 0.8479\n218/400, Train_loss: 0.1662\nTrain_dice: 0.8338\n219/400, Train_loss: 0.2158\nTrain_dice: 0.7842\n220/400, Train_loss: 0.1960\nTrain_dice: 0.8040\n221/400, Train_loss: 0.1956\nTrain_dice: 0.8044\n222/400, Train_loss: 0.2534\nTrain_dice: 0.7466\n223/400, Train_loss: 0.2543\nTrain_dice: 0.7457\n224/400, Train_loss: 0.3131\nTrain_dice: 0.6869\n225/400, Train_loss: 0.1676\nTrain_dice: 0.8324\n226/400, Train_loss: 0.3329\nTrain_dice: 0.6671\n227/400, Train_loss: 0.2109\nTrain_dice: 0.7891\n228/400, Train_loss: 0.2284\nTrain_dice: 0.7716\n229/400, Train_loss: 0.1615\nTrain_dice: 0.8385\n230/400, Train_loss: 0.1915\nTrain_dice: 0.8085\n231/400, Train_loss: 0.1568\nTrain_dice: 0.8432\n232/400, Train_loss: 0.1997\nTrain_dice: 0.8003\n233/400, Train_loss: 0.4149\nTrain_dice: 0.5851\n234/400, Train_loss: 0.1814\nTrain_dice: 0.8186\n235/400, Train_loss: 0.1812\nTrain_dice: 0.8188\n236/400, Train_loss: 0.2077\nTrain_dice: 0.7923\n237/400, Train_loss: 0.1451\nTrain_dice: 0.8549\n238/400, Train_loss: 0.4262\nTrain_dice: 0.5738\n239/400, Train_loss: 0.1801\nTrain_dice: 0.8199\n240/400, Train_loss: 0.3149\nTrain_dice: 0.6851\n241/400, Train_loss: 0.2313\nTrain_dice: 0.7687\n242/400, Train_loss: 0.2193\nTrain_dice: 0.7807\n243/400, Train_loss: 0.1613\nTrain_dice: 0.8387\n244/400, Train_loss: 0.1286\nTrain_dice: 0.8714\n245/400, Train_loss: 0.2483\nTrain_dice: 0.7517\n246/400, Train_loss: 0.2408\nTrain_dice: 0.7592\n247/400, Train_loss: 0.2401\nTrain_dice: 0.7599\n248/400, Train_loss: 0.3488\nTrain_dice: 0.6512\n249/400, Train_loss: 0.1501\nTrain_dice: 0.8499\n250/400, Train_loss: 0.2220\nTrain_dice: 0.7780\n251/400, Train_loss: 0.1962\nTrain_dice: 0.8038\n252/400, Train_loss: 0.1832\nTrain_dice: 0.8168\n253/400, Train_loss: 0.3647\nTrain_dice: 0.6353\n254/400, Train_loss: 0.2782\nTrain_dice: 0.7218\n255/400, Train_loss: 0.1549\nTrain_dice: 0.8451\n256/400, Train_loss: 0.1461\nTrain_dice: 0.8539\n257/400, Train_loss: 0.2646\nTrain_dice: 0.7354\n258/400, Train_loss: 0.2460\nTrain_dice: 0.7540\n259/400, Train_loss: 0.1472\nTrain_dice: 0.8528\n260/400, Train_loss: 0.1468\nTrain_dice: 0.8532\n261/400, Train_loss: 0.1313\nTrain_dice: 0.8687\n262/400, Train_loss: 0.1553\nTrain_dice: 0.8447\n263/400, Train_loss: 0.1541\nTrain_dice: 0.8459\n264/400, Train_loss: 0.1957\nTrain_dice: 0.8043\n265/400, Train_loss: 0.1403\nTrain_dice: 0.8597\n266/400, Train_loss: 0.1510\nTrain_dice: 0.8490\n267/400, Train_loss: 0.1496\nTrain_dice: 0.8504\n268/400, Train_loss: 0.2206\nTrain_dice: 0.7794\n269/400, Train_loss: 0.1670\nTrain_dice: 0.8330\n270/400, Train_loss: 0.2129\nTrain_dice: 0.7871\n271/400, Train_loss: 0.2513\nTrain_dice: 0.7487\n272/400, Train_loss: 0.2154\nTrain_dice: 0.7846\n273/400, Train_loss: 0.2268\nTrain_dice: 0.7732\n274/400, Train_loss: 0.1809\nTrain_dice: 0.8191\n275/400, Train_loss: 0.1692\nTrain_dice: 0.8308\n276/400, Train_loss: 0.1886\nTrain_dice: 0.8114\n277/400, Train_loss: 0.1608\nTrain_dice: 0.8392\n278/400, Train_loss: 0.2507\nTrain_dice: 0.7493\n279/400, Train_loss: 0.1568\nTrain_dice: 0.8432\n280/400, Train_loss: 0.1649\nTrain_dice: 0.8351\n281/400, Train_loss: 0.1624\nTrain_dice: 0.8376\n282/400, Train_loss: 0.2373\nTrain_dice: 0.7627\n283/400, Train_loss: 0.1460\nTrain_dice: 0.8540\n284/400, Train_loss: 0.1296\nTrain_dice: 0.8704\n285/400, Train_loss: 0.1931\nTrain_dice: 0.8069\n286/400, Train_loss: 0.1577\nTrain_dice: 0.8423\n287/400, Train_loss: 0.1702\nTrain_dice: 0.8298\n288/400, Train_loss: 0.1646\nTrain_dice: 0.8354\n289/400, Train_loss: 0.1728\nTrain_dice: 0.8272\n290/400, Train_loss: 0.2582\nTrain_dice: 0.7418\n291/400, Train_loss: 0.2217\nTrain_dice: 0.7783\n292/400, Train_loss: 0.2979\nTrain_dice: 0.7021\n293/400, Train_loss: 0.1458\nTrain_dice: 0.8542\n294/400, Train_loss: 0.1335\nTrain_dice: 0.8665\n295/400, Train_loss: 0.2076\nTrain_dice: 0.7924\n296/400, Train_loss: 0.1833\nTrain_dice: 0.8167\n297/400, Train_loss: 0.2765\nTrain_dice: 0.7235\n298/400, Train_loss: 0.1314\nTrain_dice: 0.8686\n299/400, Train_loss: 0.3283\nTrain_dice: 0.6717\n300/400, Train_loss: 0.1907\nTrain_dice: 0.8093\n301/400, Train_loss: 0.2011\nTrain_dice: 0.7989\n302/400, Train_loss: 0.2649\nTrain_dice: 0.7351\n303/400, Train_loss: 0.2209\nTrain_dice: 0.7791\n304/400, Train_loss: 0.2397\nTrain_dice: 0.7603\n305/400, Train_loss: 0.2152\nTrain_dice: 0.7848\n306/400, Train_loss: 0.2327\nTrain_dice: 0.7673\n307/400, Train_loss: 0.2828\nTrain_dice: 0.7172\n308/400, Train_loss: 0.1717\nTrain_dice: 0.8283\n309/400, Train_loss: 0.1518\nTrain_dice: 0.8482\n310/400, Train_loss: 0.1672\nTrain_dice: 0.8328\n311/400, Train_loss: 0.1606\nTrain_dice: 0.8394\n312/400, Train_loss: 0.2973\nTrain_dice: 0.7027\n313/400, Train_loss: 0.1594\nTrain_dice: 0.8406\n314/400, Train_loss: 0.1812\nTrain_dice: 0.8188\n315/400, Train_loss: 0.3275\nTrain_dice: 0.6725\n316/400, Train_loss: 0.1976\nTrain_dice: 0.8024\n317/400, Train_loss: 0.3387\nTrain_dice: 0.6613\n318/400, Train_loss: 0.2017\nTrain_dice: 0.7983\n319/400, Train_loss: 0.2375\nTrain_dice: 0.7625\n320/400, Train_loss: 0.2008\nTrain_dice: 0.7992\n321/400, Train_loss: 0.2479\nTrain_dice: 0.7521\n322/400, Train_loss: 0.2334\nTrain_dice: 0.7666\n323/400, Train_loss: 0.1823\nTrain_dice: 0.8177\n324/400, Train_loss: 0.2778\nTrain_dice: 0.7222\n325/400, Train_loss: 0.2581\nTrain_dice: 0.7419\n326/400, Train_loss: 0.1753\nTrain_dice: 0.8247\n327/400, Train_loss: 0.1229\nTrain_dice: 0.8771\n328/400, Train_loss: 0.1515\nTrain_dice: 0.8485\n329/400, Train_loss: 0.3147\nTrain_dice: 0.6853\n330/400, Train_loss: 0.2144\nTrain_dice: 0.7856\n331/400, Train_loss: 0.1638\nTrain_dice: 0.8362\n332/400, Train_loss: 0.2156\nTrain_dice: 0.7844\n333/400, Train_loss: 0.1657\nTrain_dice: 0.8343\n334/400, Train_loss: 0.1671\nTrain_dice: 0.8329\n335/400, Train_loss: 0.1766\nTrain_dice: 0.8234\n336/400, Train_loss: 0.1632\nTrain_dice: 0.8368\n337/400, Train_loss: 0.3016\nTrain_dice: 0.6984\n338/400, Train_loss: 0.3672\nTrain_dice: 0.6328\n339/400, Train_loss: 0.1933\nTrain_dice: 0.8067\n340/400, Train_loss: 0.2983\nTrain_dice: 0.7017\n341/400, Train_loss: 0.2188\nTrain_dice: 0.7812\n342/400, Train_loss: 0.2900\nTrain_dice: 0.7100\n343/400, Train_loss: 0.1635\nTrain_dice: 0.8365\n344/400, Train_loss: 0.2087\nTrain_dice: 0.7913\n345/400, Train_loss: 0.1649\nTrain_dice: 0.8351\n346/400, Train_loss: 0.1796\nTrain_dice: 0.8204\n347/400, Train_loss: 0.1271\nTrain_dice: 0.8729\n348/400, Train_loss: 0.2447\nTrain_dice: 0.7553\n349/400, Train_loss: 0.1345\nTrain_dice: 0.8655\n350/400, Train_loss: 0.3012\nTrain_dice: 0.6988\n351/400, Train_loss: 0.1432\nTrain_dice: 0.8568\n352/400, Train_loss: 0.2563\nTrain_dice: 0.7437\n353/400, Train_loss: 0.2648\nTrain_dice: 0.7352\n354/400, Train_loss: 0.1625\nTrain_dice: 0.8375\n355/400, Train_loss: 0.2735\nTrain_dice: 0.7265\n356/400, Train_loss: 0.1589\nTrain_dice: 0.8411\n357/400, Train_loss: 0.1986\nTrain_dice: 0.8014\n358/400, Train_loss: 0.2104\nTrain_dice: 0.7896\n359/400, Train_loss: 0.2843\nTrain_dice: 0.7157\n360/400, Train_loss: 0.2357\nTrain_dice: 0.7643\n361/400, Train_loss: 0.1588\nTrain_dice: 0.8412\n362/400, Train_loss: 0.1628\nTrain_dice: 0.8372\n363/400, Train_loss: 0.2920\nTrain_dice: 0.7080\n364/400, Train_loss: 0.3219\nTrain_dice: 0.6781\n365/400, Train_loss: 0.1633\nTrain_dice: 0.8367\n366/400, Train_loss: 0.2796\nTrain_dice: 0.7204\n367/400, Train_loss: 0.1967\nTrain_dice: 0.8033\n368/400, Train_loss: 0.2653\nTrain_dice: 0.7347\n369/400, Train_loss: 0.1614\nTrain_dice: 0.8386\n370/400, Train_loss: 0.1792\nTrain_dice: 0.8208\n371/400, Train_loss: 0.1755\nTrain_dice: 0.8245\n372/400, Train_loss: 0.2889\nTrain_dice: 0.7111\n373/400, Train_loss: 0.2081\nTrain_dice: 0.7919\n374/400, Train_loss: 0.1968\nTrain_dice: 0.8032\n375/400, Train_loss: 0.1651\nTrain_dice: 0.8349\n376/400, Train_loss: 0.2457\nTrain_dice: 0.7543\n377/400, Train_loss: 0.1438\nTrain_dice: 0.8562\n378/400, Train_loss: 0.1942\nTrain_dice: 0.8058\n379/400, Train_loss: 0.1682\nTrain_dice: 0.8318\n380/400, Train_loss: 0.2604\nTrain_dice: 0.7396\n381/400, Train_loss: 0.2270\nTrain_dice: 0.7730\n382/400, Train_loss: 0.2564\nTrain_dice: 0.7436\n383/400, Train_loss: 0.2217\nTrain_dice: 0.7783\n384/400, Train_loss: 0.2129\nTrain_dice: 0.7871\n385/400, Train_loss: 0.1377\nTrain_dice: 0.8623\n386/400, Train_loss: 0.2773\nTrain_dice: 0.7227\n387/400, Train_loss: 0.2245\nTrain_dice: 0.7755\n388/400, Train_loss: 0.1256\nTrain_dice: 0.8744\n389/400, Train_loss: 0.2412\nTrain_dice: 0.7588\n390/400, Train_loss: 0.1726\nTrain_dice: 0.8274\n391/400, Train_loss: 0.1267\nTrain_dice: 0.8733\n392/400, Train_loss: 0.1461\nTrain_dice: 0.8539\n393/400, Train_loss: 0.2558\nTrain_dice: 0.7442\n394/400, Train_loss: 0.1788\nTrain_dice: 0.8212\n395/400, Train_loss: 0.1592\nTrain_dice: 0.8408\n396/400, Train_loss: 0.1384\nTrain_dice: 0.8616\n397/400, Train_loss: 0.1537\nTrain_dice: 0.8463\n398/400, Train_loss: 0.1460\nTrain_dice: 0.8540\n399/400, Train_loss: 0.1578\nTrain_dice: 0.8422\n400/400, Train_loss: 0.1336\nTrain_dice: 0.8664\n--------------------\nEpoch_loss: 0.2176\nEpoch_metric: 0.7824\ntest_loss_epoch: 0.3996\ntest_dice_epoch: 0.6004\ncurrent epoch: 5 current mean dice: 0.5992\nbest mean dice: 0.6004 at epoch: 5\n----------\nepoch 6/600\n1/400, Train_loss: 0.1624\nTrain_dice: 0.8376\n2/400, Train_loss: 0.1447\nTrain_dice: 0.8553\n3/400, Train_loss: 0.1619\nTrain_dice: 0.8381\n4/400, Train_loss: 0.1264\nTrain_dice: 0.8736\n5/400, Train_loss: 0.2549\nTrain_dice: 0.7451\n6/400, Train_loss: 0.1462\nTrain_dice: 0.8538\n7/400, Train_loss: 0.2658\nTrain_dice: 0.7342\n8/400, Train_loss: 0.1620\nTrain_dice: 0.8380\n9/400, Train_loss: 0.1575\nTrain_dice: 0.8425\n10/400, Train_loss: 0.2425\nTrain_dice: 0.7575\n11/400, Train_loss: 0.1336\nTrain_dice: 0.8664\n12/400, Train_loss: 0.1627\nTrain_dice: 0.8373\n13/400, Train_loss: 0.1509\nTrain_dice: 0.8491\n14/400, Train_loss: 0.1611\nTrain_dice: 0.8389\n15/400, Train_loss: 0.1539\nTrain_dice: 0.8461\n16/400, Train_loss: 0.1697\nTrain_dice: 0.8303\n17/400, Train_loss: 0.2211\nTrain_dice: 0.7789\n18/400, Train_loss: 0.1789\nTrain_dice: 0.8211\n19/400, Train_loss: 0.2883\nTrain_dice: 0.7117\n20/400, Train_loss: 0.1647\nTrain_dice: 0.8353\n21/400, Train_loss: 0.1798\nTrain_dice: 0.8202\n22/400, Train_loss: 0.1139\nTrain_dice: 0.8861\n23/400, Train_loss: 0.1897\nTrain_dice: 0.8103\n24/400, Train_loss: 0.1658\nTrain_dice: 0.8342\n25/400, Train_loss: 0.1389\nTrain_dice: 0.8611\n26/400, Train_loss: 0.1396\nTrain_dice: 0.8604\n27/400, Train_loss: 0.1433\nTrain_dice: 0.8567\n28/400, Train_loss: 0.1734\nTrain_dice: 0.8266\n29/400, Train_loss: 0.1870\nTrain_dice: 0.8130\n30/400, Train_loss: 0.1457\nTrain_dice: 0.8543\n31/400, Train_loss: 0.3382\nTrain_dice: 0.6618\n32/400, Train_loss: 0.1681\nTrain_dice: 0.8319\n33/400, Train_loss: 0.1650\nTrain_dice: 0.8350\n34/400, Train_loss: 0.1457\nTrain_dice: 0.8543\n35/400, Train_loss: 0.2128\nTrain_dice: 0.7872\n36/400, Train_loss: 0.1345\nTrain_dice: 0.8655\n37/400, Train_loss: 0.2427\nTrain_dice: 0.7573\n38/400, Train_loss: 0.1678\nTrain_dice: 0.8322\n39/400, Train_loss: 0.1973\nTrain_dice: 0.8027\n40/400, Train_loss: 0.1295\nTrain_dice: 0.8705\n41/400, Train_loss: 0.1833\nTrain_dice: 0.8167\n42/400, Train_loss: 0.3204\nTrain_dice: 0.6796\n43/400, Train_loss: 0.2133\nTrain_dice: 0.7867\n44/400, Train_loss: 0.2512\nTrain_dice: 0.7488\n45/400, Train_loss: 0.1250\nTrain_dice: 0.8750\n46/400, Train_loss: 0.1384\nTrain_dice: 0.8616\n47/400, Train_loss: 0.1809\nTrain_dice: 0.8191\n48/400, Train_loss: 0.1476\nTrain_dice: 0.8524\n49/400, Train_loss: 0.2540\nTrain_dice: 0.7460\n50/400, Train_loss: 0.1567\nTrain_dice: 0.8433\n51/400, Train_loss: 0.1375\nTrain_dice: 0.8625\n52/400, Train_loss: 0.1415\nTrain_dice: 0.8585\n53/400, Train_loss: 0.2606\nTrain_dice: 0.7394\n54/400, Train_loss: 0.1407\nTrain_dice: 0.8593\n55/400, Train_loss: 0.1899\nTrain_dice: 0.8101\n56/400, Train_loss: 0.1577\nTrain_dice: 0.8423\n57/400, Train_loss: 0.2144\nTrain_dice: 0.7856\n58/400, Train_loss: 0.2129\nTrain_dice: 0.7871\n59/400, Train_loss: 0.1543\nTrain_dice: 0.8457\n60/400, Train_loss: 0.1557\nTrain_dice: 0.8443\n61/400, Train_loss: 0.1452\nTrain_dice: 0.8548\n62/400, Train_loss: 0.4132\nTrain_dice: 0.5868\n63/400, Train_loss: 0.1313\nTrain_dice: 0.8687\n64/400, Train_loss: 0.1370\nTrain_dice: 0.8630\n65/400, Train_loss: 0.2310\nTrain_dice: 0.7690\n66/400, Train_loss: 0.2440\nTrain_dice: 0.7560\n67/400, Train_loss: 0.1551\nTrain_dice: 0.8449\n68/400, Train_loss: 0.2795\nTrain_dice: 0.7205\n69/400, Train_loss: 0.2607\nTrain_dice: 0.7393\n","output_type":"stream"}]},{"cell_type":"code","source":"#loss_function = DiceCELoss(to_onehot_y=True, sigmoid=True, squared_pred=True, ce_weight=calculate_weights(1792651250,2510860).to(device))\nloss_function = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\noptimizer = torch.optim.Adam(model.parameters(), 1e-5, weight_decay=1e-5, amsgrad=True)\n\ntrain(model, data_in, loss_function, optimizer, 600, model_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = UNet().to(device)\n\n#loss_function = DiceCELoss(to_onehot_y=True, sigmoid=True, squared_pred=True, ce_weight=calculate_weights(1792651250,2510860).to(device))\nloss_function = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\noptimizer = torch.optim.Adam(model.parameters(), 1e-5, weight_decay=1e-5, amsgrad=True)\n\n\ntrain(model, data_in, loss_function, optimizer, 600, model_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass BifurcatingLayer(nn.Module):\n    def __init__(self, channels):\n        super(BifurcatingLayer, self).__init__()\n        self.main_path = nn.Conv3d(channels, channels, kernel_size=1)\n        self.bifurcation1 = nn.Conv3d(channels, channels, kernel_size=1)\n        self.bifurcation2 = nn.Conv3d(channels, channels, kernel_size=1)\n\n    def forward(self, x):\n        main = self.main_path(x)\n        b1 = self.bifurcation1(x)\n        b2 = self.bifurcation2(x)\n        return torch.where(torch.abs(main) > 1, b1, b2)\n\nclass ADN(nn.Module):\n    def __init__(self, channels, dropout_prob=0.0):\n        super(ADN, self).__init__()\n        self.N = nn.BatchNorm3d(channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.D = nn.Dropout(p=dropout_prob, inplace=False)\n        self.A = BifurcatingLayer(channels)\n\n    def forward(self, x):\n        x = self.N(x)\n        x = self.D(x)\n        x = self.A(x)\n        return x\n\n# The rest of the code remains the same\n\nclass Convolution(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, is_transposed=False, use_adn=True):\n        super(Convolution, self).__init__()\n        if is_transposed:\n            self.conv = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride, padding, output_padding=stride//2)\n        else:\n            self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)\n        self.use_adn = use_adn\n        if use_adn:\n            self.adn = ADN(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.use_adn:\n            x = self.adn(x)\n        return x\n\nclass ResidualUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualUnit, self).__init__()\n        self.conv = nn.Sequential(\n            Convolution(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n            Convolution(out_channels, out_channels, kernel_size=3, stride=1, padding=1, use_adn=False)\n        )\n        if stride != 1 or in_channels != out_channels:\n            self.residual = Convolution(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, use_adn=False)\n        else:\n            self.residual = nn.Identity()\n\n    def forward(self, x):\n        return self.conv(x) + self.residual(x)\n\nclass SkipConnection(nn.Module):\n    def __init__(self, submodule):\n        super(SkipConnection, self).__init__()\n        self.submodule = submodule\n\n    def forward(self, x):\n        return torch.cat([x, self.submodule(x)], dim=1)\n\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n        self.model = nn.Sequential(\n            ResidualUnit(4, 16, stride=2),\n            SkipConnection(\n                nn.Sequential(\n                    ResidualUnit(16, 32, stride=2),\n                    SkipConnection(\n                        nn.Sequential(\n                            ResidualUnit(32, 64, stride=2),\n                            SkipConnection(\n                                nn.Sequential(\n                                    ResidualUnit(64, 128, stride=2),\n                                    SkipConnection(\n                                        ResidualUnit(128, 256, stride=1)\n                                    ),\n                                    nn.Sequential(\n                                        Convolution(384, 64, kernel_size=3, stride=2, padding=1, is_transposed=True),\n                                        ResidualUnit(64, 64)\n                                    )\n                                )\n                            ),\n                            nn.Sequential(\n                                Convolution(128, 32, kernel_size=3, stride=2, padding=1, is_transposed=True),\n                                ResidualUnit(32, 32)\n                            )\n                        )\n                    ),\n                    nn.Sequential(\n                        Convolution(64, 16, kernel_size=3, stride=2, padding=1, is_transposed=True),\n                        ResidualUnit(16, 16)\n                    )\n                )\n            ),\n            nn.Sequential(\n                Convolution(32, 2, kernel_size=3, stride=2, padding=1, is_transposed=True),\n                ResidualUnit(2, 2)\n            )\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# Create the model\nmodel = UNet()\n\n# Print the model structure\nprint(model)\n\n# You can now use this model for training or inference\n# For example:\ninput_tensor = torch.randn(1, 4, 64, 64, 64)  # Batch size 1, 4 input channels, 64x64x64 volume\noutput = model(input_tensor)\nprint(output.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t6WDx_4olmac","outputId":"4ae783c1-b06f-4f9f-d8c5-6acf2954495a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" if cache:\n    train_ds = CacheDataset(data = train_files, transform = train_transforms, cache_rate = 1.0)\n    train_loader = DataLoader(train_ds, batch_size = 1)\n\n    test_ds = CacheDataset(data = test_files, transform = test_transform, cache_rate = 1.0)\n    test_loader = DataLoader(test_ds, batch_size = 1)\n\n    return train_loader, test_loader","metadata":{"id":"_Nr2enswl9Xv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir('/kaggle/working/results')\nos.mkdir('/kaggle/working/results/results')","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:46:41.528876Z","iopub.execute_input":"2024-09-04T13:46:41.529189Z","iopub.status.idle":"2024-09-04T13:46:41.533932Z","shell.execute_reply.started":"2024-09-04T13:46:41.529160Z","shell.execute_reply":"2024-09-04T13:46:41.533189Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from monai.inferers import sliding_window_inference","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"in_dir = '/kaggle/working/'\nmodel_dir = '/kaggle/working/results/results'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = np.load(os.path.join(model_dir, 'loss_train.npy'))\ntrain_metric = np.load(os.path.join(model_dir, 'metric_train.npy'))\ntest_loss = np.load(os.path.join(model_dir, 'loss_test.npy'))\ntest_metric = np.load(os.path.join(model_dir, 'metric_test.npy'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(\"Results 14th Aufust Brain Tumour with unet\", (12, 6))\nplt.subplot(2, 2, 1)\nplt.title(\"Train dice loss\")\nx = [i + 1 for i in range(len(train_loss))]\ny = train_loss\nplt.xlabel(\"epoch\")\nplt.plot(x, y)\n\nplt.subplot(2, 2, 2)\nplt.title(\"Train metric DICE\")\nx = [i + 1 for i in range(len(train_metric))]\ny = train_metric\nplt.xlabel(\"epoch\")\nplt.plot(x, y)\n\nplt.subplot(2, 2, 3)\nplt.title(\"Test dice loss\")\nx = [i + 1 for i in range(len(test_loss))]\ny = test_loss\nplt.xlabel(\"epoch\")\nplt.plot(x, y)\n\nplt.subplot(2, 2, 4)\nplt.title(\"Test metric DICE\")\nx = [i + 1 for i in range(len(test_metric))]\ny = test_metric\nplt.xlabel(\"epoch\")\nplt.plot(x, y)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_train_volumes = sorted(glob(os.path.join(in_dir, \"TrainVolumes\", \"*.nii.gz\")))\npath_train_segmentation = sorted(glob(os.path.join(in_dir, \"TrainSegmentation\", \"*.nii.gz\")))\n\npath_test_volumes = sorted(glob(os.path.join(in_dir, \"TestVolumes\", \"*.nii.gz\")))\npath_test_segmentation = sorted(glob(os.path.join(in_dir, \"TestSegmentation\", \"*.nii.gz\")))\n\ntrain_files = [{\"vol\": image_name, \"seg\": label_name} for image_name, label_name in zip(path_train_volumes, path_train_segmentation)]\ntest_files = [{\"vol\": image_name, \"seg\": label_name} for image_name, label_name in zip(path_test_volumes, path_test_segmentation)]\ntest_files = test_files[0:9]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"  test_transforms = Compose([\n      LoadImaged(keys=[\"vol\", \"seg\"]),\n      EnsureChannelFirstd(keys=[\"vol\", \"seg\"]),\n      Spacingd(keys=[\"vol\", \"seg\"], pixdim=(1.5,1.5,1.0), mode=(\"bilinear\", \"nearest\")),\n      Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n      CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n      Resized(keys=[\"vol\", \"seg\"], spatial_size=[128,128,128]),   \n      ToTensord(keys=[\"vol\", \"seg\"])\n  ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = Dataset(data=test_files, transform=test_transforms)\ntest_loader = DataLoader(test_ds, batch_size=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\")\nmodel = UNet(\n    spatial_dims=3,\n    in_channels=4,\n    out_channels=2,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n    norm=Norm.BATCH,\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:42:15.800415Z","iopub.status.idle":"2024-09-04T13:42:15.800816Z","shell.execute_reply.started":"2024-09-04T13:42:15.800596Z","shell.execute_reply":"2024-09-04T13:42:15.800610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(\n    os.path.join(model_dir, \"best_metric_model.pth\")))\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:42:15.801973Z","iopub.status.idle":"2024-09-04T13:42:15.802424Z","shell.execute_reply.started":"2024-09-04T13:42:15.802179Z","shell.execute_reply":"2024-09-04T13:42:15.802200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sw_batch_size = 4\nroi_size = (128, 128, 64)\nwith torch.no_grad():\n    test_patient = first(test_loader)\n    t_volume = test_patient['vol']\n    #t_segmentation = test_patient['seg']\n    \n    test_outputs = sliding_window_inference(t_volume.to(device), roi_size, sw_batch_size, model)\n    sigmoid_activation = Activations(sigmoid=True)\n    test_outputs = sigmoid_activation(test_outputs)\n    test_outputs = test_outputs > 0.53\n        \n    for i in range(80, 90):\n        # plot the slice [:, :, 80]\n        plt.figure(\"check\", (18, 6))\n        plt.subplot(1, 3, 1)\n        plt.title(f\"image {i}\")\n        plt.imshow(test_patient[\"vol\"][0, 0, :, :, i], cmap=\"gray\")\n        plt.subplot(1, 3, 2)\n        plt.title(f\"label {i}\")\n        plt.imshow(test_patient[\"seg\"][0, 0, :, :, i] != 0)\n        plt.subplot(1, 3, 3)\n        plt.title(f\"output {i}\")\n        plt.imshow(test_outputs.detach().cpu()[0, 1, :, :, i])\n        plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from monai.transforms import Activations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass PitchforkActivation(nn.Module):\n    def __init__(self, bifurcation_point=0):\n        super(PitchforkActivation, self).__init__()\n        self.bifurcation_point = bifurcation_point\n\n    def forward(self, x):\n        return torch.where(x < self.bifurcation_point, \n                           -torch.pow(x - self.bifurcation_point, 3),\n                           torch.pow(x - self.bifurcation_point, 3))\n\nclass BifurcatingLayer(nn.Module):\n    def __init__(self, channels):\n        super(BifurcatingLayer, self).__init__()\n        self.main_path = nn.Conv3d(channels, channels, kernel_size=1)\n        self.bifurcation1 = nn.Conv3d(channels, channels, kernel_size=1)\n        self.bifurcation2 = nn.Conv3d(channels, channels, kernel_size=1)\n\n    def forward(self, x):\n        main = self.main_path(x)\n        b1 = self.bifurcation1(x)\n        b2 = self.bifurcation2(x)\n        return torch.where(torch.abs(main) > 1, b1, b2)\n\nclass ADN(nn.Module):\n    def __init__(self, channels, dropout_prob=0.0):\n        super(ADN, self).__init__()\n        self.N = nn.BatchNorm3d(channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.D = nn.Dropout(p=dropout_prob, inplace=False)\n        self.A = PitchforkActivation()\n        self.B = BifurcatingLayer(channels)\n\n    def forward(self, x):\n        x = self.N(x)\n        x = self.D(x)\n        x = self.A(x)\n        x = self.B(x)\n        return x\n\nclass Convolution(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, is_transposed=False, use_adn=True):\n        super(Convolution, self).__init__()\n        if is_transposed:\n            self.conv = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride, padding, output_padding=stride//2)\n        else:\n            self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)\n        self.use_adn = use_adn\n        if use_adn:\n            self.adn = ADN(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.use_adn:\n            x = self.adn(x)\n        return x\n\nclass ResidualUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualUnit, self).__init__()\n        self.conv = nn.Sequential(\n            Convolution(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n            Convolution(out_channels, out_channels, kernel_size=3, stride=1, padding=1, use_adn=False)\n        )\n        if stride != 1 or in_channels != out_channels:\n            self.residual = Convolution(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, use_adn=False)\n        else:\n            self.residual = nn.Identity()\n\n    def forward(self, x):\n        return self.conv(x) + self.residual(x)\n\nclass SkipConnection(nn.Module):\n    def __init__(self, submodule):\n        super(SkipConnection, self).__init__()\n        self.submodule = submodule\n\n    def forward(self, x):\n        return torch.cat([x, self.submodule(x)], dim=1)\n\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n        self.model = nn.Sequential(\n            ResidualUnit(4, 16, stride=2),\n            SkipConnection(\n                nn.Sequential(\n                    ResidualUnit(16, 32, stride=2),\n                    SkipConnection(\n                        nn.Sequential(\n                            ResidualUnit(32, 64, stride=2),\n                            SkipConnection(\n                                nn.Sequential(\n                                    ResidualUnit(64, 128, stride=2),\n                                    SkipConnection(\n                                        ResidualUnit(128, 256, stride=1)\n                                    ),\n                                    nn.Sequential(\n                                        Convolution(384, 64, kernel_size=3, stride=2, padding=1, is_transposed=True),\n                                        ResidualUnit(64, 64)\n                                    )\n                                )\n                            ),\n                            nn.Sequential(\n                                Convolution(128, 32, kernel_size=3, stride=2, padding=1, is_transposed=True),\n                                ResidualUnit(32, 32)\n                            )\n                        )\n                    ),\n                    nn.Sequential(\n                        Convolution(64, 16, kernel_size=3, stride=2, padding=1, is_transposed=True),\n                        ResidualUnit(16, 16)\n                    )\n                )\n            ),\n            nn.Sequential(\n                Convolution(32, 2, kernel_size=3, stride=2, padding=1, is_transposed=True),\n                ResidualUnit(2, 2)\n            )\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# Create the model\nmodel = UNet()\n\n# Print the model structure\nprint(model)\n\n# Adaptive Learning Rate Scheduler\nclass PitchforkLRScheduler(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, bifurcation_point, last_epoch=-1):\n        self.bifurcation_point = bifurcation_point\n        super(PitchforkLRScheduler, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch < self.bifurcation_point:\n            return [base_lr for base_lr in self.base_lrs]\n        else:\n            return [base_lr * (1 + abs(self.last_epoch - self.bifurcation_point))\n                    for base_lr in self.base_lrs]\n\n# Example usage:\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n# scheduler = PitchforkLRScheduler(optimizer, bifurcation_point=50)\n\n# You can now use this model for training or inference\n# For example:\n# input_tensor = torch.randn(1, 4, 64, 64, 64)  # Batch size 1, 4 input channels, 64x64x64 volume\n# output = model(input_tensor)\n# print(output.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}